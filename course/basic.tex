% !TEX TS-program = xelatex
% !TeX program = xelatex
% !TEX encoding = UTF-8
% !TEX spellcheck = en_US

%=====================================================================
\ifx\wholebook\relax\else
	\documentclass{KBook}
	\input{calls}
	\begin{document}
		\mainmatter
	
\fi
%=====================================================================
\changegraphpath{../../img/basic/}

\chapter{Basic Text Processing}

\begin{introduction}[Languages]
	%	\lettrine{A}{fin} de traiter un langage, nous commençons par sa plus petite unité : le graphème. 
	\lettrine{W}{hen} dealing with a language, we start with its smallest unit: the grapheme. 
	In computer science, graphemes along with punctuation and spacing are called characters. 
	There are operations that allow us to search for and compare strings of characters.
	By combining characters, we get a text with sentences and words. 
	Given a text, we should be able to segment it into sentences and a sentence into words. 
	The latter can be unnecessary for certain tasks, such as prepositions in information retrieval. 
	Thus, they need to be filtered before applying this task. 
	Also, words can have multiple morphological variations. 
	If we want to process a single variation, we need to normalize them. 
	In addition, we should be able to generate these variations or switch from one variation to another given a word form. 
	This chapter summarizes some tasks at the morphological level of languages (lexical analysis).
\end{introduction} 

According to Larousse, a character in computing and telecommunications is defined as: ``\textit{Any symbol (digit, letter of the alphabet, punctuation mark, etc.) used to represent data for processing or transmission purposes.}".
A word is composed of several characters according to a regular language. 
A sentence, in turn, consists of several words according to a context-free language (in most languages). 
This last proposition concerns the syntactic level.
For now, we are interested in the morphological level or what is called lexical analysis.
The points addressed in this chapter are as follows: 
\begin{itemize}
	\item Searching in the text using regular expressions. 
	Among the applications: data extraction (dates, phone numbers, email addresses, etc.).
	
	\item Comparing strings of characters using edit distance. 
	Among its applications: spell correction and approximate search.
	
	\item Text segmentation into sentences and sentences into words. 
	
	\item Text normalization to reduce word variations.
	
	\item Word formation in synthetic languages and the reverse operation.
\end{itemize}

\section{Character Processing}

Here, we consider a text as a sequence of characters. 
In a more formal way, a text can be composed using a finite state automaton where the vocabulary is the set of characters. 
To search for substrings of characters in a text, we can use regular expressions recognizing type 3 languages (regular languages) in the Chomsky hierarchy. 
Searching for sequences in a text allows us to replace or separate them (e.g., \expword{word separation}). 
Another type of search is approximate search: looking for parts almost similar to a given string. 
To do this, we must be able to compare two strings of characters by measuring the difference between them. 
One of the techniques used is edit distance.

\subsection{Regular Expressions}

A regular expression, called \keyword[R]{RegEx}, is a sequence of characters specifying a search pattern.
Several programming languages provide the ability to search and replace using regular expressions. 
To use it for search, a regular expression is transformed into a finite state automaton (FSA) which is then transformed into a deterministic FSA.
A dot in a regular expression represents any character.
If we want to search for a dot and not any character, we need to add a backslash before the dot.
We can design a complex regular expression by composing several simple regular expressions.
These are described in Table \ref{fig:exp-reg} with their meanings and examples.

\begin{table}[ht]
	\begin{tabular}{p{.1\textwidth}p{.34\textwidth}p{.46\textwidth}}
		\hline\hline
		\textbf{RE} & \textbf{Meaning} & \textbf{Example} \\
		\hline
		
		. & any character & /beg.n/ : \expword{I \underline{begun} at the \underline{begin}ning} \\
		
		\empty [aeuio] & specific characters & /[Ll][ae]/ : \expword{\underline{Le} chat mange \underline{la} sourie} \\
		
		\empty [a-e] & range of characters & /[A-Z]../ : \expword{\underline{J'a}i vu \underline{Kar}im} \\
		
		\empty [\textasciicircum aeuio] & exclude characters & /[\textasciicircum A-Z]a./ : \expword{J\underline{'ai} vu Karim} \\
		
		c? & zero or one & /colou?r/ : \expword{It is \underline{colour} or \underline{color}} \\
		
		c* & zero or more & /No*n/ : \expword{\underline{Nn}! \underline{Non}! \underline{Nooooooon}!} \\
		
		c+ & one or more & /No+n/ : \expword{Nn! \underline{Non}! \underline{Nooooooon}!} \\
		
		c\{n\} & n occurrences & /No\{3\}n/ : \expword{Nn! Non! Noon! \underline{Nooon}!} \\
		
		c\{n,m\} & n to m occurrences & /No\{1,2\}n/ : \expword{Nn! \underline{Non}! \underline{Noon}! Nooon!} \\
		
		c\{n,\} & at least n occurrences & /No\{2,\}n/ : \expword{Nn! Non! \underline{Noon}! \underline{Nooon}!} \\
		
		c\{,m\} & at most m occurrences & /No\{,2\}n/ : \expword{\underline{Nn}! \underline{Non}! \underline{Noon}! Nooon!} \\
		
		\hline 
		
		\textbackslash d & [0-9] & /\textbackslash d\{2,\}/ : \expword{The year \underline{1962}}\\
		
		\textbackslash D & [\textasciicircum 0-9] & /\textbackslash D\{2,\}/ : \expword{\underline{The year }1962}\\
		
		\textbackslash w & [a-zA-Z0-9\_] & /\textbackslash w\{2,\}/ : \expword{\underline{The} \underline{year} 1962}\\
		
		\textbackslash W & [\textasciicircum \textbackslash w] & /\textbackslash W\{2,\}/ : \expword{\underline{The year} \underline{1962}}\\
		
		\textbackslash s & [ \textbackslash r\textbackslash t\textbackslash n\textbackslash f] & /\textbackslash s+/ : \expword{\underline{The year}\underline{ }1962\underline{ }}\\
		
		\textbackslash S & [\textasciicircum \textbackslash s] & /\textbackslash S+/ : \expword{\underline{The year} \underline{1962}}\\
		
		\hline 
		
		( ) & grouping & /(bla)+/ : \expword{This is \underline{blabla}}\\
		
		\textbar & disjunction & /continu(er\textbar ation)/ : \expword{I \underline{continue} the \underline{continuation}} \\
		
		\textasciicircum  & start of text & /\textasciicircum K/ :  \expword{\underline{K}ill Karim}\\
		
		\$ & end of text & /\textbackslash .[\textasciicircum .]+\$/ :  \expword{file.tar\underline{.gz}}\\
		
		\hline\hline
	\end{tabular}
	
	\caption{Regular expressions.}
	\label{fig:exp-reg}
\end{table}

The most common use of regular expressions is searching for substrings of characters in a large text.
Most text editors and programming languages provide mechanisms to use regular expressions.
Using patterns to search for strings makes regular expressions a very powerful tool for data extraction.
For example, we can use regular expressions to extract emails from blogs and social networks.
Let's take an example of a regular expression \expword{/[a-zA-Z]\textbackslash w*(@| at )[a-zA-Z]\textbackslash w+\textbackslash .[a-zA-Z]\{2,3\}/} to search for email addresses.
The part \expword{/[a-zA-Z]/} ensures that both the username and the domain name start with a letter.
The username accepts at least one character since the first letter is followed by \expword{/\textbackslash w*/}, meaning zero or more characters of type letter, digit, or underscore.
The domain name in this regular expression accepts at least two characters since the first letter is followed by \expword{/\textbackslash w+/}, meaning one or more characters of type letter, digit, or underscore.
The username and domain name are separated either by an ``\expword{@}" or by ``\expword{ at }"; this is indicated by \expword{/(@| at )/}.
The domain extension must always be preceded by a dot; here, we use \expword{/\textbackslash ./} to indicate that it is the dot character.
Otherwise, if we used \expword{/./}, it represented any character.
The extension is composed of two to three letters.
Here are some emails that we can extract using this regular expression: 
``\expword{ab\_aries@esi.dz}", ``\expword{ab\_ARIES@eSi.dz}", ``\expword{ab\_aries at esi.dz}", ``\expword{a@es.edu}", ``\expword{a1@e2s.edu}".

To capture a string, we use grouping with the group number.
Group number "N" must be preceded by a special character, often ``\$" or ``\textbackslash".
An example of text replacement in a text editor (``kate" in our case) is illustrated in Figure \ref{fig:kate_regex}.
Here, we search for the word ``cat" to replace it with the word ``dog". 
But, we want to keep ``ch" as it is shared between the two words.
In this case, we capture the string ``ch" to keep it and replace the rest: ``at" with ``ien".


Perhaps the utility of grouping is not clear with the previous example since we have only one string to capture.
If we had a disjunction, it would be more interesting.
Let's take the example: ``\expword{J'aime les chats. Dans la cuisine, il y a un rat.}".
If we replace the string ``at" in both ``chat" and ``rat" with ``ien", we would have the following text:
``\expword{J'aime les chiens. Dans la cuisine, il y a un rien.}".
In this case, we need to test the words ``chat" and ``rat" to replace only the last part and keep the first part intact.
Several programming languages provide string replacement using \keyword[R]{RegEx}.
In Javascript, strings are assigned a ``replace" method to replace one of their parts with another string.
If we want to replace all occurrences, we should use the ``g" flag after the regular expression.
To retrieve the captured group, we use ``\$" followed by the group's order.

\begin{lstlisting}[language={[KB]Javascript}, style=codeStyle]
	let orig = "J'aime les chats. Dans la cuisine, il y a un rat.";
	let remp = orig.replace(/(ch|r)at/g, "$1ien");
\end{lstlisting}

For any use of regular expressions in Python, we need to use the ``re" module.
A regular expression is a string preceded by the ``r" indicator.
To retrieve the captured group, we use ``\textbackslash".

\begin{lstlisting}[language=Python, style=codeStyle]
	import re
	orig = "J'aime les chats. Dans la cuisine, il y a un rat."
	remp = re.sub(r'(ch|r)at', r'\1ien', orig)
\end{lstlisting}



\subsection{Distance d'édition}

Des fois, lorsque nous rédigeons en utilisant nos ordinateurs, nous faisons des erreurs de frappe ; nous pouvons insérer un caractère de plus, dupliquer un caractère, etc. 
Une telle opération est appelée ``opération d'édition". 
Nous pouvons comparer entre deux chaînes de caractères en comptant le nombre des opérations d'édition pour passer d'une chaîne originale à une autre modifiée.
Les différentes opérations d'édition sont les suivantes (où $X$ est l'ensemble des caractères du langage) :
%
\begin{itemize}
	\item \optword{Insertion} : insertion d'un caractère dans une chaîne 
	($uv \rightarrow uxv \,/\, u, v \in X^*;\, uv \in X^+;\, x \in X$).
	Par exemple, \expword{courir $ \rightarrow $ courrir, entraînement $ \rightarrow $ entraînnement}.
	
	\item \optword{Suppression} : suppression d'un caractère d'une chaîne
	($uxv \rightarrow uv \,/\, u, v \in X^*;\, uv \in X^+;\, x \in X$).
	Par exemple, \expword{héros $ \rightarrow $ héro, meilleur $ \rightarrow $ meileur}.
	
	\item \optword{Substitution} : substitution d'un caractère par un autre
	($uxv \rightarrow uyv \,/\, u, v \in X^*;\, x, y \in X;\, x \ne y$).
	Par exemple, \expword{cela $ \rightarrow $ celà, croient $ \rightarrow $ croyent }
	
	\item \optword{Transposition} : changement de l'ordre de deux caractères
	($uxwyv \rightarrow uywxv \,/\, u, v, w \in X^*;\, x, y \in X;\, x \ne y$).
	Par exemple, \expword{cueillir $ \rightarrow $ ceuillir}.
\end{itemize}

Savoir le nombre des opérations d'édition nous permet de comparer deux chaînes de caractères. 
Il existe plusieurs exemples de l'utilisation de la distance d'édition (nombre des modifications) :
\begin{itemize}
	\item \optword{Révision des fichiers} : par exemple, la commande Unix ``\expword{diff}" qui compare deux fichiers.
	\item \optword{Correction d'orthographe} : suggérer des corrections possibles d'une faute (Ex. \expword{Hunspell}).
	\item \optword{Détection du plagiat} : ici, nous utilisons des mots à la place des caractères.
	\item \optword{Filtrage de spam} : parfois, les spammeurs commettent des fautes d'orthographe intentionnellement pour tromper l'outil de détection de spam.
	\item \optword{Bio-informatique} : quantification de la similarité entre deux séquences d'ADN.
\end{itemize}

\subsubsection{Distance de Hamming}

Cette distance permet seulement la substitution.
Les chaînes doivent être de la même longueur.
Parmi ses utilisations, nous pouvons citer la détection des erreurs de transmission de données.\newline
Par exemple, \expword{D(010\underline{0}101\underline{0}01\underline{1}0, 010\underline{1}101\underline{1}01\underline{0}0) = 3}.
Pour calculer la distance, nous comparons les deux chaînes caractère par caractère pour avoir le nombre des caractères différents (voir l'algorithme \ref{algo:hamming}).

\begin{algorithm}[ht]
	\KwData{orig, modif}
	\KwResult{distance: entier}
	distance $\leftarrow$ 0\;
	
	\Pour{pos $ \in 1 \ldots |orig|$ }{
		\lSi{orig[pos] $\ne$ modif[pos]}{
			distance $\leftarrow$ distance + 1
		}
	}

	\caption{Calcul de la distance de Hamming \label{algo:hamming}}
	
\end{algorithm}


\subsubsection{Plus longue sous-séquence commune}

Cette distance permet l'insertion et la suppression.
Parmi ses utilisations, la détection des modifications dans le programme ``diff" (utilisé aussi dans ``Git").
En anglais, elle s'appelle ``Longest common subsequence" (LCS).
Étant donné deux chaînes $X$ et $Y$ avec les longueurs $n$ et $m$ respectivement, nous définissons un tableau $C[m, n]$ à deux dimensions contenant la longueur de la LCS.
L'équation \ref{eq:LCS-length} peut être utilisée afin de calculer la longueur de la plus longue sous-séquence. 
\begin{equation}
	C[i, j] =  
	\begin{cases}
		0 & \text{Si } i = 0 \text{ ou } j=0\\
		C[i-1, j-1] + 1 & \text{Si } i,j > 0 \text{ et } x_i = y_j\\
		max (C[i, j-1], C[i-1, j]) & \text{Si } i,j > 0 \text{ et } x_i \ne y_j\\
	\end{cases}
	\label{eq:LCS-length}
\end{equation}

Dans ce cas, la longueur de la plus longue sous-séquence $|LCS(X, Y)| = C[m, n]$.
La distance sera calculée en utilisant l'équation \ref{eq:LCS-distance}.
\begin{equation}
	D(X, Y) = m + n - 2 |LCS(X, Y)|
	\label{eq:LCS-distance}
\end{equation}

\subsubsection{Distance de Levenshtein}

Cette distance permet l'insertion, la suppression et la substitution.
En général, elle est utilisée dans la recherche approximative et la vérification d'orthographe.
Étant donné deux chaînes $X$ et $Y$ avec les longueurs $n$ et $m$ respectivement, nous définissons un tableau $D[m, n]$ à deux dimensions contenant la distance d'édition entre les sous-chaînes X[1..i] et Y[1..j]. 
Dans ce cas $D[0, 0] = 0$ et la distance finale sera stockée dans $D[m, n]$.
L'équation \ref{eq:lev-distance} formule comment on calcule cette distance (en utilisant la programmation dynamique).
Il faut savoir que dans cette version, le coût de la suppression et l'insertion est $1$ et le coût de la substitution est $2$.
\begin{equation}
	D[i, j] = \min 
	\begin{cases}
		D[i - 1, j] + 1 \text{ //Suppression}\\
		D[i, j-1] + 1 \text{ //Insertion}\\
		D[i-1, j-1] + \begin{cases}
			2 & \text{si } x_i \ne y_j \\
			0 & \text{sinon}
		\end{cases}
	\end{cases}
	\label{eq:lev-distance}
\end{equation}

Par exemple, \expword{D(amibe, immature) = 9}. 
Dans ce cas, ``a" et ``m" sont supprimés (1+1) ; ``i" reste le même (0) ; les caractères ``m", ``m", ``a", ``t" et ``u"  sont insérés (1+1+1+1+1), ``b" est substitué par ``r" (2) et ``e" reste le même (0).
Le calcul est illustré dans la figure \ref{fig:laven-distance} où les flèches représentent d'où vient la valeur et les cellules colorées représentent le chemin choisi.
Bien sûr, nous pouvons choisir un autre chemin qui donne une autre interprétation.
Pour aider le choix du chemin, nous pouvons utiliser des probabilités des opérations d'édition.
Par exemple, dans la correction d'orthographe, des lettres sont plus probables d'être remplacées par d'autres (celles adjacentes dans le clavier).
\begin{figure}[ht]
	\centering
	\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|}
		\hline
		&\bfseries \# &\bfseries i &\bfseries m &\bfseries m &\bfseries a &\bfseries t &\bfseries u &\bfseries r &\bfseries e \\
		\hline
		\bfseries \# & 0 & $ \leftarrow $ 1 & $ \leftarrow $ 2 & $ \leftarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 & $ \leftarrow $ 8\\
		\hline
		\bfseries a & \cellcolor{green!25} $ \uparrow $ 1 & $ \nwarrow\leftarrow\uparrow $ 2 & $ \nwarrow\leftarrow\uparrow $ 3 & $ \nwarrow\leftarrow\uparrow $ 4 & $ \nwarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 \\
		\hline
		\bfseries m & \cellcolor{green!25} $ \uparrow $ 2 & $ \nwarrow\leftarrow\uparrow $ 3 & $\nwarrow $ 2 & $\nwarrow\leftarrow $ 3 & $\leftarrow\uparrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow\leftarrow\uparrow $ 6 & $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8\\
		\hline
		\bfseries i & $ \uparrow $ 3 & \cellcolor{green!25} $ \nwarrow $ 2 & \cellcolor{green!25} $\leftarrow\uparrow $ 3 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 4 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 5 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 6 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8 & $\nwarrow\leftarrow\uparrow $ 9\\
		\hline
		\bfseries b & $ \uparrow $ 4 & $ \uparrow $ 3 & $\nwarrow\leftarrow\uparrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow\leftarrow\uparrow $ 6 & $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 9 & $\nwarrow\leftarrow\uparrow $ 10\\
		\hline
		\bfseries e & $ \uparrow $ 5 & $ \uparrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow\leftarrow\uparrow $ 6 & $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8 & $\nwarrow\leftarrow\uparrow $ 9 & $\nwarrow\leftarrow\uparrow $ 10 & \cellcolor{green!25} $\nwarrow $ 9\\
		\hline
	\end{tabular}
	\caption[Exemple de calcul de distance de Levenshtein.]{Exemple de calcul de distance de Levenshtein des mots ``amibe"  et ``immature" ; figure inspirée de \cite{2019-jurafsky-martin}.}
	\label{fig:laven-distance}
\end{figure}
%\begin{figure}[ht]
%	\centering
%	\hgraphpage[.75\textwidth]{exp-levenshtein_.pdf}
%	\caption[Exemple de calcul de distance de Levenshtein]{Exemple de calcul de distance de Levenshtein \cite{2019-jurafsky-martin} \label{fig:laven-distance}}
%\end{figure}

\subsubsection{Distance de Damerau–Levenshtein}

Cette distance permet l'insertion, la suppression, la substitution et la transposition entre deux caractères adjacents.
En général, elle est utilisée dans la vérification d'orthographe.
Cette distance est calculée comme celle de Levenstein, en ajoutant la transposition entre deux caractères adjacents.
L'équation \ref{eq:lev-dem-distance} représente comment calculer cette distance.
Dans cette version, nous avons essayé d'attribuer le poids $1$ pour toutes les opérations d'édition.
\begin{equation}
D[i, j] = \min 
	\begin{cases}
		D[i - 1, j] + 1 \text{ //Suppression}\\
		D[i, j-1] + 1 \text{ //Insertion}\\
		D[i-1, j-1] + \begin{cases}
			1 & \text{si } x_i \ne y_j \text{ //Substitution}\\
			0 & \text{sinon}
		\end{cases}\\
		D[i-2, j-2] + 1 \text{ si } x_i = y_{j-1} \text{ et } x_{i-1} = y_j \text{ //Transposition}\\
	\end{cases}
	\label{eq:lev-dem-distance}
\end{equation}

Par exemple, \expword{D(amibe, immature) = 6}. 
Dans ce cas, ``i" et ``m" sont insérés (1+1) ; ``a" est transposé avec ``m" (1) ; ``t" est inséré (1) ; ``i" est substitué par ``u" (1) ;  ``b" est substitué par ``r" (1) et ``e" reste le même (0).
Le calcul est illustré dans la figure \ref{fig:dam-laven-distance} où les flèches représentent d'où vient la valeur et les cellules colorées représentent le chemin choisi.
Lorsqu'il y a une transposition, nous sautons deux cellules diagonales.
\begin{figure}[ht]
	\centering
	\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|}
		\hline
		&\bfseries \# &\bfseries i &\bfseries m &\bfseries m &\bfseries a &\bfseries t &\bfseries u &\bfseries r &\bfseries e \\
		\hline
		\bfseries \# & 0 & \cellcolor{green!25} $ \leftarrow $ 1 & \cellcolor{green!25} $ \leftarrow $ 2 & $ \leftarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 & $ \leftarrow $ 8\\
		\hline
		\bfseries a & $ \uparrow $ 1 & $ \nwarrow $ 1 & $ \nwarrow\leftarrow $ 2 & $ \nwarrow\leftarrow $ 3 & $ \nwarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 \\
		\hline
		\bfseries m & $ \uparrow $ 2 & $ \nwarrow\uparrow $ 2 & $\nwarrow $ 1 & $\nwarrow\leftarrow $ 2 & \cellcolor{green!25} $\leftrightharpoons\leftarrow $ 3 & \cellcolor{green!25} $\leftarrow $ 4 & $\leftarrow $ 5 & $\leftarrow $ 6 & $\leftarrow $ 7\\
		\hline
		\bfseries i & $ \uparrow $ 3 & $ \nwarrow $ 2 & $\leftrightharpoons\uparrow $ 2 & $\nwarrow\leftarrow\uparrow $ 3 & $\nwarrow $ 3 &  $\nwarrow\leftarrow $ 4 & \cellcolor{green!25} $\nwarrow\leftarrow $ 5 & $\nwarrow\leftarrow $ 6 & $\nwarrow\leftarrow $ 7\\
		\hline
		\bfseries b & $ \uparrow $ 4 & $ \uparrow $ 3 & $\nwarrow\uparrow $ 3 & $\nwarrow $ 3 & $\nwarrow\leftarrow\uparrow $ 4 & $\nwarrow $ 4 & $\nwarrow\leftarrow $ 5 & \cellcolor{green!25} $\nwarrow\leftarrow $ 6 & $\nwarrow\leftarrow $ 7\\
		\hline
		\bfseries e & $ \uparrow $ 5 & $ \uparrow $ 4 & $\nwarrow\uparrow $ 4 & $\nwarrow\uparrow $ 5 & $\nwarrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow $ 5 & $\nwarrow\leftarrow $ 6 & \cellcolor{green!25} $\nwarrow $ 6\\
		\hline
	\end{tabular}
	\caption[Exemple de calcul de distance de Damerau–Levenshtein.]{Exemple de calcul de distance de Damerau–Levenshtein des mots ``amibe"  et ``immature".}
	\label{fig:dam-laven-distance}
\end{figure}

\subsubsection{Distance de Jaro}

Cette distance permet seulement la transposition.
En réalité c'est une mesure de similarité qui retourne une valeur entre $0$ (pas de similarité) et $1$ (similaires). 
Elle est utilisée pour le calcul de la similarité entre les entités nommées, etc.
Étant donné deux chaînes $X$ et $Y$ avec les longueurs $n$ et $m$ respectivement, nous calculons le nombre des caractères correspondants $c$ et le nombre des transpositions $t$. 
La distance de Jaro est calculée selon l'équation \ref{eq:jaro-distance}.
\begin{equation}
	D(X, Y) = 
	\begin{cases}
	0 & \text{si } c = 0\\
	\frac{1}{3} (\frac{c}{m} + \frac{c}{n} + \frac{c-t}{c}) & \text{sinon}
	\end{cases}
	\label{eq:jaro-distance}
\end{equation}

Le nombre des caractères correspondants $c$  est le nombre des caractères identiques de $X$ et de $Y$ avec un éloignement $ e= \max (m, n)/2 - 1$. Dans ce cas, si nous soyons dans le caractère $i$ (allant de $0$ jusqu'à $n-1$) du mot $X$, nous commencerions la comparaison par le caractère $j=\max(0, i - e)$ du mot $Y$ et nous terminerions par le caractère $j=\min(i+e, m-1)$.
Dans cette opération, nous pouvons préparer deux vecteurs des booléens qui indiquent la position du caractère ayant un correspondant dans l'autre mot.
%
Le nombre des transpositions $t$ est calculé en comparant le i\textsuperscript{ème} caractère correspondant de $X$ avec le i\textsuperscript{ème} caractère correspondant de $Y$. 
Nous comptons le nombre des fois $x_i \ne y_i$  et nous le divisons par deux (voir l'algorithme \ref{algo:jaro-transpo}).
\vspace{-6pt}
\begin{algorithm}[ht]
	\KwData{Xmatch, Ymatch : vecteurs de booléens}
	\KwResult{t: entier}
	t $\leftarrow$ 0 ; j $\leftarrow$ 0\;
	
	\Pour{i $ \in 0 \ldots m$ }{
		\Si{Xmatch[i] = Vrai}{
			\lTq{Ymatch[j] $\ne$ Vrai}{
				j $\leftarrow$ j + 1
			}
			
		    \lSi{$x_i \ne y_j$}{
		    	t $\leftarrow$ t + 1
		    }
	    	j $\leftarrow$ j + 1\;
		}
	}
    
    t $\leftarrow$ t/2\;
    
    \caption{Calcul du nombre de transpositions entre deux mots X et Y dans la distance de Jaro \label{algo:jaro-transpo}}
	
\end{algorithm}

Prenons l'exemple de ``amibe"  et ``immature" ayant les tailles 5 et 8 respectivement.
L'éloignement sera $e=\max(5, 8)/2 - 1 = 3$.
La figure \ref{fig:jaro} représente le calcul de la distance entre ces deux mots en utilisant les deux ordres possibles.
Dans la matrice, $0$ veut dire ``Faux" et $1$  veut dire ``Vrai" (correspondance). 
Les cases en gras représentent la plage de recherche en utilisant l'éloignement de 3. 
Les cases soulignées représentent l'intersection entre la ième correspondance de X et la ième correspondance de Y.
Les correspondances soulignées des vecteurs des deux mots représentent des transpositions. 

\begin{figure}[ht]
	
\begin{minipage}{0.45\textwidth}
\small
\begin{tabular}{|l|l|l|l|l|l|l|l|}
	\cline{1-6}
	  & a & m & i & b & e & \multicolumn{2}{l}{ }\\
	\cline{1-6}\cline{8-8}
	i & \underline{\textbf{0}} & \textbf{0} & \textbf{1} & \textbf{0} & \textbf{0} & & 1\\
	\cline{1-6}\cline{8-8}
	m & \textbf{0} & \underline{\textbf{1}} & \textbf{0} & \textbf{0} & \textbf{0} & & 1\\
	\cline{1-6}\cline{8-8}
	m & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	a & \textbf{1} & \textbf{0} & \underline{\textbf{0}} & \textbf{0} & \textbf{0} & & 1\\
	\cline{1-6}\cline{8-8}
	t & 0 & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	u & 0 & 0 & \textbf{0} & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	r & 0 & 0 & 0 & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	e & 0 & 0 & 0 & 0 & \underline{\textbf{1}} & & 1\\
	\cline{1-6}\cline{8-8}
	\multicolumn{8}{l}{ }\\
	\cline{2-6}
	 \multicolumn{1}{l|}{} & \underline{1} & 1 & \underline{1} & 0 & 1 & \multicolumn{2}{l}{ }\\
	\cline{2-6}
\end{tabular}

\end{minipage}
\begin{minipage}{0.45\textwidth}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
	\cline{1-9}
	  & i & m & m & a & t & u & r & e & \multicolumn{2}{l}{ }\\
	\cline{1-9}\cline{11-11}
	a & \underline{\textbf{0}} & \textbf{0} & \textbf{0} & \textbf{1} & 0 & 0 & 0 & 0 & & 1\\
	\cline{1-9}\cline{11-11}
	m & \textbf{0} & \underline{\textbf{1}} & \textbf{0} & \textbf{0} & \textbf{0} & 0 & 0 & 0 & & 1\\
	\cline{1-9}\cline{11-11}
	i & \textbf{1} & \textbf{0} & \textbf{0} & \underline{\textbf{0}} & \textbf{0} & \textbf{0} & 0 & 0 & & 1\\
	\cline{1-9}\cline{11-11}
	b & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & 0 & & 0\\
	\cline{1-9}\cline{11-11}
	e & 0 & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \underline{\textbf{1}} & & 1\\
	\cline{1-9}\cline{11-11}
	\multicolumn{11}{l}{ }\\
	\cline{2-9}
	\multicolumn{1}{l|}{} & \underline{1} & 1 & 0 & \underline{1} & 0 & 0 & 0 & 1 & \multicolumn{2}{l}{ }\\
	\cline{2-9}
\end{tabular}
\end{minipage}
\caption[Exemple de calcul de la distance de Jaro.]{Exemple de calcul de la distance de Jaro des mots ``amibe"  et ``immature".}
\label{fig:jaro}
\end{figure}

%===================================================================================
\section{Segmentation du texte}
%===================================================================================

Un texte peut être traité lorsqu'il est segmenté en passages de petites tailles ; ils peuvent être des chapitres, des paragraphes, des phrases ou des mots selon la granularité voulue.
Dans plusieurs langues, les phrases sont délimitées par un point ou une marque spécifique, et les mots sont délimités par des espaces. 
Même dans ces langues, le délimiteur peut être utilisé pour d'autres fins. 
Il existe des langues où il n'y a pas de délimiteur des phrases, des mots ou des deux.

\subsection{Délimitation de la phrase}

Afin de séparer les phrases dans un format semi-structuré, comme HTML, nous pouvons utiliser des marqueurs comme la balise \expword{\textless P\textgreater}.
Mais, lorsqu'il s'agit d'un texte non structuré, plusieurs langues utilisent des marqueurs comme le point, point d'exclamation et point d'interrogation pour marquer la fin d'une phrase. 
Dans ce cas, nous pouvons utiliser une expression régulière simple \expword{/[.?!]/} pour délimiter les phrases dans des langues comme français, anglais, etc.
Des fois, nous voulons séparer les phrases longues avec des clauses séparées par des virgules.
Aussi, les guillemets peuvent causer un problème : est-ce que la phrase dedans est séparée ou elle est une continuation de la clause principale ?
Par exemple, \expword{Il a dit : ``Je suis fatigué." en retournant à son lit.}.
Le point dans ces langues n'est pas toujours utilisé pour séparer les phrases. 
Il peut être utilisé dans les nombres : \expword{123,456.78 (style américain) 123.456,78 (style européen)}.
Les abréviations comme ``Mr.", ``Dr." et ``etc." contiennent des points et peuvent se trouver au milieu de la phrase. 
Ils peuvent, aussi, se trouver à la fin ; donc, il faut vraiment être capable de détecter les abréviations et s'ils marquent la fin de la phrase ou non. 
Si tous ça ne parait pas problématique, nous pouvons toujours essayer de séparer les phrase du thaï. 
Cette langue n'utilise pas des marqueurs pour séparer les phrases.

Lorsque le marqueur de phrase est réservé pour d'autres utilisations, il y a toujours des facteurs qui aident le lecteur à décider si c'est un délimiteur de phrase ou non.
Ces facteurs peuvent être extraits en observant comment la langage définit la fin de la phrase. 
Quelques facteurs contextuels ont été proposés et utilisés pour la segmentation des phrases \cite{10-palmer} :
\begin{itemize}
	\item \optword{La casse} : les phrases commencent toujours par un majuscule. 
	Mais, ce n'est pas toujours le cas ; nous pouvons trouver une abréviation suivie par un nom propre (Ex. ``\expword{M. Aries}"). 
	En plus, ce n'est pas garanti que l'écrivain respecte les règles d'écriture. 
	Nous pouvons voir cela, par exemple, dans les réseaux sociaux où plusieurs règles sont abandonnées.
	
	\item \optword{Noms propres} : les noms propres commencent par un majuscule ; ils peuvent ne pas être le début.
	Dans l'exemple précédent (``\expword{M. Aries}"), nous pouvons déduire que le point ne représente pas une séparation vu que les deux mots commencent par un majuscule et le deuxième est un nom propre. 
	Dans ce cas, la probabilité que le premier soit une abréviation est grande surtout si le mot est au milieu de la phrase.
	
	\item \optword{Catégorie grammaticale} : les catégories des mots qui entourent le point peuvent aider la décision (limite ou non). 
	En fait,  \citet{97-palmer-hearst} ont été capables d'améliorer la détection des limites des phrases en utilisant les catégories grammaticales des deux mots avant et après le point avec un algorithme d'apprentissage automatique. 
	
	\item \optword{Longueur du mot} : les abréviations sont moins longues.
	Revenons toujours à l'exemple précédent, il est clair que ``M" n'est pas vraiment un mot. 
	
	\item \optword{Préfixes et suffixes} : les mots avec des affixes sont moins probables d'être des abréviations.
	
	\item \optword{Classes des abréviations} : les abréviations peuvent figurer à la fin de la phrase. 
	Mais, il y a un ensemble d'abréviations qui sont toujours suivies par un autre mot, comme par exemple ``Mr.".
	\citet{89-riley,97-reynar-ratnaparkhi} divisent les abréviations en deux catégories : les titres (qui ne peuvent pas être à la fin de phrase (Ex. ``\expword{Mr.}", ``\expword{Dr.}", etc.) et les indicatifs corporatifs (qui peuvent être à la fin de phrase (Ex. ``\expword{Corp.}", ``\expword{S.P.A.}", etc.).
\end{itemize}

%Les différents algorithmes de détection de limites
La détection automatique des limites d'une phrase peut être accomplie en utilisant des règles manuelles ou en utilisant l'apprentissage automatique. 
Dans la première approche, nous pouvons utiliser des expressions régulières pour détecter les délimiteurs. 
Ensuite, nous pouvons utiliser une liste des abréviations pour améliorer la décision. 
Les règles peuvent être enrichies en introduisant les facteurs discutés précédemment. 
Afin d'éviter l'écriture manuelle de ces règles, nous pouvons utiliser un algorithme d'apprentissage automatique qui classe le point comme délimiteur/non-délimiteur en se basant sur ces même règles.


\subsection{Séparation des mots}

Plusieurs langues (arabe, français, anglais, etc.) utilisent l'espace comme délimiteur des mots.
Une expression régulière simple comme \expword{/[ ]+/} peut être utilisée pour séparer les mots. 
Mais, des fois nous voulons récupérer une expression avec plusieurs mots ; comme le cas des dates, des nombres, etc. 
Un exemple des nombres est ``\expword{neuf cent quarante}" ; cette expression peut être considérée comme un seul token dans certains traitements.
Dans des langues, l'apostrophe peut être une source d'ambigüité. 
Dans l'anglais, l'apostrophe peut être utilisée avec un ``\textit{s}" dans la forme possessive (\expword{Karim's thesis}), dans les contractions (\expword{she's, it's, I'm, we've}) ou dans le pluriel de certains mots (\expword{I.D.'s, 1980's}). 
Dans le français, il y a pas mal d'exemples de contractions : la contraction des articles (\expword{l'homme, c'était}), la contraction des pronoms (\expword{j'ai, je l'ai}), et autres formes (\expword{n'y, qu'ils, d'ailleurs}).
Certaines langues utilisent des mots composés, soit par composition ou par trait d'union. 
Dans l'allemand, il est commun d'utiliser la composition des mots: nom-nom (\expword{Lebensversicherung: assurance vie}), adverbe-nom (\expword{Nichtraucher: non-fumeur}), et préposition-nom (\expword{Nachkriegszeit: période d'après-guerre}). 
D'autres langues utilisent le trait d'union, comme l'anglais (\expword{end-of-file, classification-based}) et le français (\expword{va-t-il, c'est-à-dire, celui-ci}). 
Il existe des langues, comme le japonais, qui n'utilisent pas de marqueurs pour séparer les mots (\expword{今年は本当に忙しかったです。}).


Il existe deux approches pour la séparation des mots : par règles et statistique. 
L'approche par règles utilise principalement les expressions régulières en se basant sur les règles morphologiques.
Elle peut aussi utiliser des listes des mots. 
Par exemple, dans une langue de type \keyword{Scriptio Continua} comme le japonais et le chinois, nous pouvons utiliser un dictionnaire et comparer les mots en commençant par la fin en prenant la séquence la plus longue.
L'approche statistique utilise un modèle de langage pour calculer la probabilité qu'un caractère marque la fin d'un mot. 
Les modèles de langages seront présentés dans le chapitre suivant. 

%Approches
%\begin{itemize}
%	\item Par règles : en utilisant des expressions régulières 
%	\begin{itemize}
%		\item \url{https://www.nltk.org/api/nltk.tokenize.html}
%		\item \url{https://nlp.stanford.edu/software/tokenizer.shtml}
%		\item \url{https://spacy.io/}
%		\item \url{https://github.com/kariminf/jslingua}
%		\item \url{https://github.com/linuxscout/pyarabic}
%	\end{itemize}
%	\item Statistique : en utilisant un modèle de langue pour calculer la probabilité qu'un caractère marque la  fin d'un mot 
%	\begin{itemize}
%		\item \url{https://nlp.stanford.edu/software/segmenter.html}
%		\item \url{https://opennlp.apache.org/}
%	\end{itemize}
%\end{itemize}

%===================================================================================
\section{Normalisation du texte}
%===================================================================================

Un texte peut contenir des variations du même terme. 
Dans des tâches qui se basent sur les statistiques sur des mots (comme la recherche d'information), nous devons traiter ces variations comme un seul token. 
Même dans des tâches comme la compréhension de la langue, nous avons besoin de trouver des formes standards des dialectes (comme ``\expword{ain't}").
La transformation du texte à une forme canonique, en général, se fait en utilisant des expressions régulières pour chercher les variations et un dictionnaire pour chercher leurs formes canoniques. 
Parmi les variations qui ont besoin d'être normalisées, nous pouvons citer :
\begin{itemize}
	\item \optword{Acronymes et les abréviations} : des fois, nous trouvons plusieurs variations d'une même abréviation.
	Dans ce cas, il faut choisir une seule variation pour les représenter. 
	Par exemple, \expword{US \textrightarrow\ USA, U.S.A. \textrightarrow\ USA}.
	Dans les tâches où nous avons besoin d'une compréhension plus approfondie, nous devons chercher la version longue. 
	Par exemple, \expword{M. \textrightarrow\ Monsieur}.
	
	\item \optword{Valeurs numériques} (dates et nombres) : 
	selon la tâche, nous devons unifier le format des valeurs numériques. 
	Des fois, nous avons besoin de trouver la forme textuelle (\expword{1205 DZD \textrightarrow\ Mille deux cents cinq dinars algériens}). 
	Cela est utile, par exemple, dans le cas de la synthèse de parole où l'appareil doit prononcer ce qui est écrit.
	Lorsque nous avons plusieurs sources de textes, nous tombons sur plusieurs variations des dates. 
	Dans ce cas, nous pouvons utiliser le standard \keyword{ISO 8601} (\expword{12 Janvier 1986, 12.01.86 \textrightarrow\ 1986-01-12}).
	Dans les tâches qui s'intéressent aux statistiques (Ex. nombre des dates dans un texte), nous n'avons pas besoin de garder les formes numériques. 
	Dans ce cas, nous pouvons garder seulement le type de ces formes (\expword{12 Janvier 1986 \textrightarrow\ DATE, kariminfo0@gmail.com \textrightarrow\ EMAIL}).
	
	\item \optword{Majuscules et Minuscules} : dans la plupart des cas, nous n'avons pas besoin de garder les mots en majuscules (\expword{Texte \textrightarrow\ texte}). 
	Mais, il faut faire attention lorsque nous avons besoin de garder cette information pour des tâches comme la segmentation du texte, ou pour différencier les nom propres (\expword{Will}).
	
	\item \optword{Diacritiques} : comme le point précédent, les diacritiques peuvent être supprimées lorsque la tâche ne dépend pas sur elles. 
	Par exemple, dans le français, nous appelons cela : désaccentuation (\expword{système \textrightarrow\ systeme}).
	Dans l'arable, nous l'appelons : dé-vocalisation (\expword{\<yadrusu> \textrightarrow\ \<ydrs>}). 
	Dans des tâches qui ont besoin de vocalisation, comme le traitement des poèmes, nous devons garder les diacritiques. 
	En fait, nous devons appliquer l'opération inverse (vocalisation du texte).
	
	\item \optword{Contractions} : en général, nous trouvons ces formes beaucoup plus sur les réseaux sociaux. 
	Mais, elles peuvent aussi être une règle standard de la langue. 
	Par exemple, \expword{y'll \textrightarrow\ you all, s'il \textrightarrow\ si il}. 
	Dans le cas du dernier exemple, ça va aider dans la tâche de séparation des mots (tokenization). 
	
	\item \optword{Encodage} : il faut utiliser le même encodage supporté dans le traitement. 
	L'encodage le plus célèbre est \keyword[U]{UTF-8}.

\end{itemize}


%===================================================================================
\section{Filtrage du texte}
%===================================================================================

Le texte peut contenir des caractères, des mots et des expressions qui peuvent entraver son traitement. 
Pour faciliter ce dernier, il faut supprimer le bruit. 
Un exemple très commun est la présence des caractères spéciaux, comme les caractères non imprimables, dans le texte. 
Les mots contenant ces caractères ne sont pas considérés les mêmes que les mots sans ces caractères. 
En général, ces caractères sont d'origine des pages web ou des PDFs (extraction du texte à partir des PDFs ou autres formes d'images).
Les mots clés des formats textuelles est un exemple des mots à filtrer. 
Nous pouvons trouver ça dans les balises de certaines formats semi-structurées comme HTML, XML, etc.


Les \optword{mots vides} représentent les mots non significatifs comme les prépositions, articles et les pronoms.
La suppression des mots vides peut être utilisée si plusieurs mots dans le document ne contribuent pas particulièrement dans la description de son contenu.
Afin d'augmenter la performance du système, plusieurs tâches (comme la recherche d'information et de résumé automatique) font appel à cette technique.
Mais, nous pouvons tomber sur des cas particuliers où cela peut causer des problèmes. 
Par exemple, la phrase ``\expword{To be or not to be}" sera totalement ignorée vu que tous ses mots sont des mots vides.
Le nom propre ``\expword{Will}" peut être ignoré par confusion avec le verbe ``to be" en future.


%===================================================================================
\section{Morphologie}
%===================================================================================

Nous avons vu dans le chapitre précédent qu'il existe pas mal de langues qui permettent la formation des mots en utilisant la flexion (ex. \expword{conjugaison}) et la dérivation (ex. \expword{nominalisation}). 
La formation des mots la plus utilisée est l'affixation en suivant certains règles. 
Par exemple, pour conjuguer le verbe ``\expword{étudier}" avec le pronom ``\expword{nous}" en présent, nous supprimons le suffixe ``\expword{er}" pour avoir le radical ``\expword{étudu}" et nous ajoutons le suffixe ``\expword{ons}". 
L'automatisation de cette tâche peut aider plusieurs applications, comme la génération du langage naturel (anglais : NLG). 
La tâche inverse consiste à trouver une forme standard des différentes variations ; c'est une technique de normalisation.
Elle peut aider dans des tâches comme la recherche d'information et la compréhension du langage naturel (anglais : NLU).

\subsection{Formation des mots}

Dans les langues synthétiques, nous pouvons former les mots en utilisant la flexion ou la dérivation. 
La flexion génère des variations morphologiques d'un mot selon les traits grammaticaux (nombre, genre, etc.). 
Les deux types de flexion sont la conjugaison des verbes et la déclinaison des noms, pronoms, adjectifs et déterminants.
Quant à la dérivation, les mots créés forment un nouveau lexèmes (un sens différent du mot original) ou ils appartiennent à une autre catégorie grammaticale. 
Un exemple d'un nouveau lexème,  \expword{couper \textrightarrow\ découper, \<`ml> \textrightarrow\ \<ist`ml>}.
Le changement de catégorie peut être dû à la nominalisation (\expword{classer \textrightarrow\ classement, classeur ; \<darasa> \textrightarrow\ \<darsuN, madrasaTuN, mudarrisuN, dArisuN>}) ou l'adjectif (\expword{fatiguer \textrightarrow\ fatigant}), etc.


La formation des mots suit des règles bien définies, donc le problème peut être résolu en utilisant un automate à état fini. 
Bien sûr, il existe des cas spéciaux qui peuvent simplement être stockés dans un fichier. 
L'approche par règles est beaucoup plus utilisée dans ce cas, mais nous pouvons trouver des recherches qui utilisent le niveau caractère pour appendre à générer des formes d'un mot donné, comme le projet MLConjug\footnote{MLConjug : \url{https://github.com/SekouD/mlconjug} [visité le 2021-09-08]}. 
D'un point de vu, l'apprentissage automatique doit être utilisé pour résoudre les problèmes vraiment difficiles (en général, niveau syntaxique, sémantique, et pragmatique). 
Autre que l'approche statistique, plusieurs méthodes traditionnelles ont été utilisées pour la formation des mots. 
Dans le contexte du conjugaison automatique des verbes, nous pouvons citer :
\begin{itemize}
	\item \optword{Base de données} : dans cette solution, nous stockons tous les verbes dans une base de donnée avec les différentes variations possibles. 
	Le point fort de cette solution est que nous puissions vérifier si un verbe appartient à la langue. 
	Aussi, en arabe, les verbes peuvent avoir les mêmes lettres ; la seule différence est en vocalisation (diacritiques).
	Malgré ces points forts, cette solution est vraiment difficile à être implémentée ; nous devons chercher tous les verbes et toutes les variations possibles. 
	
	\item \optword{Modèles (\textit{template})} : dans cette solution, nous stockons les conjugaisons de certains verbes comme modèles et une autre liste de tous les verbes de la langue avec leurs modèles respectifs.
	C'est la forme la plus utilisée en français (par exemple, le verbe ``\expword{sourire} a comme modèle le verbe ``\expword{rire}"). 
	Elle est similaire à la solution précédente, mais avec moins d'espace de stockage.
	
	\item \optword{Règles} : dans cette solution, nous utilisons des règles SI-SINON et des expressions régulières.
	Un exemple de cette méthode peut être trouvé dans le projet Qutrub\footnote{Qutrub : \url{https://github.com/linuxscout/qutrub} [visité le 2021-09-08]} pour la conjugaison automatique de l'arabe.  
	Un autre exemple de ce type est celui de JsLingua\footnote{JsLingua : \url{https://github.com/kariminf/jslingua} [visité le 2021-09-08]} pour la conjugaison des verbes en arabe, anglais, français et japonais. 
	L'avantage est que nous n'avons pas besoin de créer une base de données ; la solution est plus rapide. 
	Mais, nous devons gérer beaucoup de règles et aussi nous aurions besoin d'une base de données si nous voulions vérifier le type du verbe (il peut y avoir des verbes confondus).

\end{itemize}

\subsection{Réduction des formes}

Il y a deux types réduits d'une forme d'un mot : radical et lemme. 
Le radical est un morphème qui peut ne pas être un mot du vocabulaire, or le lemme est un mot qui représente le lexème.
La radicalisation (racinisation, en anglais : stemming) est l'opération de supprimer les affixes afin d'obtenir un radical (racine, en anglais : stem). 
Par exemple, \expword{chercher \textrightarrow\ cherch}. 
Cette tâche est rapide et préférée dans des tâches comme la recherche d'information où nous avons besoin de plus de vitesse d'exécution même au détriment de l'exactitude. 
Quelques techniques pour la radicalisation sont les suivantes :
\begin{itemize}
	\item \optword{Base de données} : Stocker tous les termes et leurs racines dans une table. 
	\item \optword{Statistiques} : Utiliser un modèle de langage (comme les \keywordpl[N]{N-Gramme}) pour estimer la position de troncation.
	\item \optword{Règles} : Utiliser un ensemble de règles condition/action afin de détecter les affixes et les tronquer. 
	L'algorithme le plus connu est celui de Porter \cite{1980-porter} pour l'anglais. 
	Il existe un framework très connu, appelé SnowBall\footnote{SnowBall : \url{https://snowballstem.org/} [visité le 2021-09-08]}, pour réaliser des racinateurs de ce genre. 
	Dans ce framework, plusieurs conditions sont utilisées : sur la racine, sur l'affixe ou sur la règle. 
	Une condition sur la racine peut être la longueur, la fin, si elle contient des voyelles, etc.
	Exemple, \expword{(*v*) Y \textrightarrow\ I : happy \textrightarrow\ happi, sky \textrightarrow\ sky}.
	Une condition sur l'affixe peut être ``il n'y a que le suffixe".
	Exemple, \expword{SSES \textrightarrow\ SS, ATIONAL \textrightarrow\ ATE}.
	Une condition sur la règle peut être : la désactivation de certaines règles si une a été exécutée.
\end{itemize}

La lemmatisation (en anglais : lemmatization) cherche la forme canonique d'un mot appelée ``lemme" (en anglais : lemma).
Exemple, \expword{comprennent \textrightarrow\ comprendre, better \textrightarrow\ good}
Cette tâche est plus difficile que celle de radicalisation, puisque nous avons besoin du contexte du mot (Ex. \expword{saw \textrightarrow\ (V) see ou (N) saw}). 
\begin{itemize}
	\item \optword{Bases lexicales} : Ici, nous utilisons la radicalisation avec d'autres règles afin de chercher une forme dans une liste des lemmes possibles (un dictionnaire).
	Un exemple de ce type de lemmatisation est la lemmatisation morphy de Wordnet (voir l'algorithme \ref{algo:morphy}).
	
	\item \optword{Apprentissage automatique} : Nous essayons d'apprendre le lemme des mots en se basant sur des critères comme leurs catégories grammaticales.
	L'outil OpenNLP\footnote{OpenNLP lemmatisation : \url{https://opennlp.apache.org/docs/1.8.0/manual/opennlp.html\#tools.lemmatizer} [visité le 2021-09-08]} implémente une version statistique de lemmatisation.

\end{itemize}

\begin{algorithm}[H]
		\KwData{mot, catégorie}
		\KwResult{liste des lemmes possibles}
		
		\Si{mot $ \in $  list\_exceptions[catégorie]}{
			\Return chercher\_dans\_dictionnaire(\{mot\} $ \cup $ list\_exceptions[catégorie][mot])\;
		}
		
		formes = \{mot\}
		
		\Tq{formes $ \ne \emptyset $}{
			formes = supprimer\_les\_affixes(formes, catégorie)\;
			
			résultats = chercher\_dans\_dictionnaire(\{mot\} $ \cup $ formes)\;
			
			\Si{résultats $ \ne \emptyset $ }{
				\Return résultats \;
			}
		}
		
		\Return $ \emptyset $\;
		
		\caption{Lemmatisation "morphy" de Wordnet \label{algo:morphy}}
		
	\end{algorithm}

\sectioni{Discussion}
%\begin{discussion}
	La morphologie est le niveau le plus simple dans le traitement d'un langage. 
	Les tâches dans ce niveau sont beaucoup plus basées sur le caractère (graphème) qui est l'unité la plus basique dans le système d'écriture.  
	La plupart des problèmes peuvent être résolus avec un automate à état fini ; d'où l'utilisation des expressions régulières. 
	Ces dernières peuvent être utilisées pour rechercher des mots, extraire les phrases et les mots, extraire des parties du mot (racine), etc. 
	Toutes ces tâches reviennent à la recherche d'un segment dans le texte. 
	Pour une recherche approximative, des méthodes de comparaison dans le niveau caractère (distance d'édition) sont utilisées. 
	
	Le texte se compose des unités qui peuvent être des chapitres, des phrases, des mots ou des caractères.
	Le mot est l'unité la plus petite ayant un sens. 
	Donc, afin de traiter un texte, il faut le décomposer en petites unités  afin de traiter les unités plus grandes, etc. 
	D'où l'importance de la segmentation du texte. 
	Ces unités peuvent avoir le même sens, mais plusieurs variations. 
	Dans la compréhension du texte, nous essayons de représenter le texte d'une manière plus abstraite.
	Donc, une variation peut être représentée par un mot représentant plus des caractéristiques comme le genre, le nombre, etc. 
	Quelques mots doivent être filtrés puisqu'ils sont considérés comme du bruit.
	
	Certes, les tâches de ce niveau peuvent être simples. 
	Mais, elles sont vraiment primordiales pour la réussite des tâches plus évoluées. 
	Ces tâches sont vraiment dépendantes à la langue traitée.
	Tellement elles sont simples, nous pouvons implémenter des outils pour n'importe quelle langue en sachant les règles morphologiques. 
	Malheureusement, pas toutes les langues fournissent de tels outils.  
	C'est la responsabilité des gens qui parlent ces langues à les développer dans un monde de plus en plus numérique.
	
%\end{discussion}

\sectioni{Ressources supplémentaires}
%\begin{ressources}
	
\subsubsection*{Exercices}

\begin{enumerate}
	\item Donner l'expression régulière qui cherche les mots ``\textbf{il}" (singulier et pluriel). 
	Comme par exemple : \expword{Il a passé par son lieu de travail et il a dit : ``j'ai devenu vieil ; ils n'ont plus besoin de moi".}
	\item Étant donné un log, on veut afficher les lignes qui commencent par ``\textbf{Error}" ; qui contiennent un nombre commençant par un chiffre autre que zéro, ayant des 3 à 5 zéros consécutifs et se terminant par un chiffre autre que zéro ; et qui se terminent par ``\textbf{...}"
	\item Donner l'expression régulière qui cherche les mots contenant les lettres ``l", ``i" et ``n" dans cet ordre. Le début du mot peut être en majuscule, le reste du mot est en minuscule. Par exemple, \expword{lion, Linux, violine, absolution, Aladin, ...}
	\item Afin d'écrire le mot "Rassemblement",  on a commis l'erreur suivante : "Rasenlbement". Indiquer la (les) position(s) de chaque opération d'édition par rapport au mot correcte (Si l'opération n'existe pas, écrire 0. La transposition et la substitution sont prioritaires, c-à-d. on ne doit pas les considérer comme des opérations d'insertion/suppression) :
	
	\begin{tabular}{|lll|}
		\hline 
		Insertion : ............ & & Substitution : ............ \\
		Suppression : ............ & & Transposition : ............ \\
		\hline
	\end{tabular}
	
	\item Calculer les distances de Hamming et de  Levenstein des deux mots suivants : ``tray" et ``tary".
	Indiquer les différentes opérations d'édition pour chaque distance. Refaire la même chose pour la distance de Lavenstein (poids de substitution = 1).
	
	\item Quelles sont les caractéristiques suffisantes pour détecter la limite des phrases dans le texte ``\textbf{I met Mr. Karim. He is at Hassiba Ave. He went to buy a PC.}" (Ave.=avenue [FR: rue]), étant donné qu'on a appliqué une phase de normalisation ? Cette dernière transforme les abréviations/acronymes vers leurs versions longues en gardant le point lorsque le mot suivant commence par une majuscule et l'abréviation peut se produire à la fin de la phrase (les caractéristiques utiles mais pas nécessaires dans ce texte sont considérées comme erronées) :
	
	\begin{tabular}{|lll|}
		\hline 
		\Square\ Casse (après ``.") & \Square\ Catégorie grammaticale (avant ``.") & \Square\ Longueur du mot (avant ``.")\\
		\Square\ Nom propre (avant ``.")& \Square\ Catégorie grammaticale (après ``.")& \Square\ Longueur du mot (après ``.")\\
		\Square\ Nom propre (après ``.") & \Square\ Classes des abréviations& \Square\ Aucune ; le ``." est suffisant\\
		\hline
	\end{tabular}

	\item Choisir, pour chaque tâche, l'opération (les opérations)  de réduction de forme utilisée(s) souvent (avec élimination des affixes) :
	
	\begin{tabular}{|llll|}
		\hline 
		Tâche & Lemmatisation & Racinisation & Aucune\\
		\hline
		Recherche d'informations (RI) & \Square & \Square & \Square \\
		Compréhension de la langue (NLU) & \Square & \Square & \Square \\
		\hline
	\end{tabular}
	
\end{enumerate}


\subsubsection*{Tutoriels}

Il existe plusieurs tutoriels accessibles à partir du répertoire Github (CH02).
Stanford CoreNLP est un outil de TALN qui supporte l'arabe, le chinois, l'anglais, le français, l'allemand et l'espagnol.
Il est implémenté en java où les tâches sont conçues sous formes des pipelines.
Dans ce tutoriel, nous présentons la segmentation et la lemmatisation en utilisant cet outil.

LangPi est un autre outil implémenté en java pour TALN.
Il a été développé par l'auteur de cet ouvrage principalement pour faciliter le prétraitement des textes.
Les tâches de prétraitement (normalisation, segmentation, radicalisation et filtrage des mots vides) sont accessibles via des interfaces facilitant l'ajout des nouvelles langues.
Actuellement, cet outil supporte plus de 40 langues.
Dans ce tutoriels, nous présentons comment pré-traiter un texte avec cet outil.

Apache OpenNLP est un outil implémenté en java pour TALN.
Il fournit des interfaces pour déférentes tâches où on peut entraîner un modèle pour une certaine langue.
Dans ce tutoriel, nous présentons le modèle de détection des langues : étant donnée un texte, on détecte sa langue.
Nous avons essayé d'entraîner un modèle pour différencier entre le langage binaire, décimal et hexadécimal. 
Le dataset d'entraînement est un peu petit ; donc, le modèle ne sera pas performant.
Ensuite, nous présentons la segmentation des phrases et des mots, ainsi que l'entraînement d'un modèle de segmentation des phrases.

NLTK est un outil très connu, présentant plusieurs tâches de TALN en python.
Dans ce tutoriel, nous présentons la segmentation des phrases, mots, syllabes et tweets.
L'outil supporte plusieurs langues pour les tâches de filtrages des mots vides, la radicalisation  et la lemmatisation. 
Ces tâches sont présentées en plus des différentes distances d'édition.


Spacy est un outil en python où les tâches sont représentées sous forme des pipelines.
Chaque langue présente des pipelines possibles qu'on stocke sous forme d'un modèle après entraînement.
Dans ce tutoriel, nous utilisons le modèle de l'anglais pour la segmentation (phrases et mots), le filtrage des mots vides et la lemmatisation.



\subsubsection*{TP : fouille des contacts}

On veut récupérer les adresses émail, les réseaux sociaux et les numéros de téléphone qui se figurent dans les pages "contactez nous" des sites web en Algérie. 
Le problème est que ces pages représentent ces informations de plusieurs façons.
Pour récupérer et unifier la forme de ces informations, on va utiliser les expressions régulières. 

L'énoncé complet du TP ainsi que les codes et les données sont téléchargeables à partir du répertoire Github.
Le TP est implémenté complètement à partir de zéro (from scratch) : recherche des contactes, évaluation, etc. 
L'étudiant doit seulement introduire des expressions régulières afin d'améliorer le score F1.
Les langages de programmation disponibles (pour l'instant) sont : Java, Javascript/nodejs et Python.


%\end{ressources}

%=====================================================================
\ifx\wholebook\relax\else
% \cleardoublepage
% \bibliographystyle{../use/ESIbib}
% \bibliography{../bib/RATstat}
	\end{document}
\fi
%===================================================================== 
