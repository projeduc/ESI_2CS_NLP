{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0628fc",
   "metadata": {},
   "source": [
    "# Text preprocessing using CoreNLP\n",
    "\n",
    "Available models are: arabic, chinese, english, french, german and spanish.\n",
    "Stanford CoreNLP affords a pipeline of tasks. \n",
    "In this tutorial, we will choose only the tasks of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0680a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pom\n",
    "dependencies:\n",
    "    - edu.stanford.nlp:stanford-corenlp:4.2.2\n",
    "    - groupId: edu.stanford.nlp\n",
    "      artifactId: stanford-corenlp\n",
    "      version: 4.2.2\n",
    "      classifier: models\n",
    "    - groupId: edu.stanford.nlp\n",
    "      artifactId: stanford-corenlp\n",
    "      version: 4.2.2\n",
    "      classifier: models-arabic\n",
    "\n",
    "# <dependencies>\n",
    "# <dependency>\n",
    "#     <groupId>edu.stanford.nlp</groupId>\n",
    "#     <artifactId>stanford-corenlp</artifactId>\n",
    "#     <version>4.0.0</version>\n",
    "# </dependency>\n",
    "# <dependency>\n",
    "#     <groupId>edu.stanford.nlp</groupId>\n",
    "#     <artifactId>stanford-corenlp</artifactId>\n",
    "#     <version>4.0.0</version>\n",
    "#     <classifier>models</classifier>\n",
    "# </dependency>\n",
    "# </dependencies>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35887a7",
   "metadata": {},
   "source": [
    "## I. Preprocessing pipeline\n",
    "\n",
    "Here, we will show how to choose pipeline tasks and launch the pipeline.\n",
    "The sentence split \"ssplit\" depends on word tokenization \"tokenize\". \n",
    "Lemmatization \"lemma\" depends on tokenization and part of speech annotation \"pos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f8fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "\n",
    "import edu.stanford.nlp.ling.*;\n",
    "import edu.stanford.nlp.ie.util.*;\n",
    "import edu.stanford.nlp.pipeline.*;\n",
    "import edu.stanford.nlp.semgraph.*;\n",
    "import edu.stanford.nlp.trees.*;\n",
    "import java.util.*;\n",
    "\n",
    "String text = \"This is a text written by Mr. Aries. It uses U.S. english to illustrate sentence tokenization.\";\n",
    "\n",
    "// set up pipeline properties\n",
    "Properties props = new Properties();\n",
    "\n",
    "// set the list of annotators to run\n",
    "props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma\");\n",
    "\n",
    "// build pipeline\n",
    "StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n",
    "\n",
    "// create a document object\n",
    "CoreDocument document = new CoreDocument(text);\n",
    "\n",
    "// annnotate the document\n",
    "pipeline.annotate(document);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660647d3",
   "metadata": {},
   "source": [
    "## II. Get tokens\n",
    "\n",
    "A \"CoreDocument\" is composed of a list of \"CoreLabel\". \n",
    "This list can be obtained using the method \"tokens()\". \n",
    "A \"CoreLabel\" affords many properties:\n",
    "- word(): to get the word; if the \"tokenize\" task has been chosen.\n",
    "- lemma(): to get the lemma; if the \"lemma\" task has been chosen.\n",
    "- ner(): to get the class of the named entity; if the \"ner\" task has been chosen.\n",
    "- tag(): to get the word's tag; if the \"tag\" task has been chosen. \n",
    "\n",
    "Here, we will just use \"word()\" to recover the word of each \"CoreLabel\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c57ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This, is, a, text, written, by, Mr., Aries, ., It, uses, U.S., english, to, illustrate, sentence, tokenization, .]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%java\n",
    "import java.util.*;\n",
    "\n",
    "List<String> words = new ArrayList<String>();\n",
    "\n",
    "for (CoreLabel token: document.tokens()){\n",
    "    words.add(token.word());\n",
    "}\n",
    "\n",
    "words;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0479a12",
   "metadata": {},
   "source": [
    "## III. Get sentences\n",
    "\n",
    "A \"CoreDocument\" is composed of a list of \"CoreSentence\". \n",
    "This list can be obtained using the method \"sentences()\". \n",
    "A \"CoreSentence\" affords many properties:\n",
    "- word(): to get the word; if the \"tokenize\" task has been chosen.\n",
    "- lemma(): to get the lemma; if the \"lemma\" task has been chosen.\n",
    "- ner(): to get the class of the named entity; if the \"ner\" task has been chosen.\n",
    "- tag(): to get the word's tag; if the \"tag\" task has been chosen. \n",
    "\n",
    "Here, we will just use \"word()\" to recover the word of each \"CoreLabel\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8503fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Sentences ------------\n",
      "[This is a text written by Mr. Aries., It uses U.S. english to illustrate sentence tokenization.]\n",
      "---------- Words in each sentence ------------\n",
      "[[This, is, a, text, written, by, Mr., Aries, .], [It, uses, U.S., english, to, illustrate, sentence, tokenization, .]]\n"
     ]
    }
   ],
   "source": [
    "%%java\n",
    "import java.util.*;\n",
    "\n",
    "//to get sentences as texts\n",
    "List<String> sentences = new ArrayList<String>();\n",
    "//to get sentences as tokens\n",
    "List<List<String>> sentencesWords = new ArrayList<>();\n",
    "\n",
    "for (CoreSentence sentence: document.sentences()){\n",
    "    sentences.add(sentence.text());\n",
    "    List<String> words = new ArrayList<String>();\n",
    "    sentencesWords.add(words);\n",
    "    for(CoreLabel token: sentence.tokens()){\n",
    "        words.add(token.word());\n",
    "    }\n",
    "}\n",
    "\n",
    "System.out.println(\"---------- Sentences ------------\");\n",
    "System.out.println(sentences);\n",
    "\n",
    "System.out.println(\"---------- Words in each sentence ------------\");\n",
    "System.out.println(sentencesWords);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698388d3",
   "metadata": {},
   "source": [
    "## IV. Get lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a872ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[this, be, a, text, write, by, Mr., Aries, ., it, use, U.S., english, to, illustrate, sentence, tokenization, .]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.*;\n",
    "\n",
    "List<String> lemmas = new ArrayList<String>();\n",
    "\n",
    "for (CoreLabel token: document.tokens()){\n",
    "    lemmas.add(token.lemma());\n",
    "}\n",
    "\n",
    "lemmas;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ff817",
   "metadata": {},
   "source": [
    "## V. Other languages\n",
    "\n",
    "We will try arabic. Lemmatization is not afforded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457fc891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "أنا ذاهب إلى السوق.\n",
      "انا, ذاهب, الى, السوق, ., \n",
      "-------------------------------------\n",
      "هل تريد أن أحضر لك شيء ما؟\n",
      "هل, تريد, ان, احضر, ل, ك, شيء, ما, ?, \n",
      "-------------------------------------\n",
      "هكذا إذن!\n",
      "هكذا, اذن, !, \n",
      "-------------------------------------\n",
      "نلتقي بعد أن أعود.\n",
      "نلتقي, بعد, ان, اعود, ., \n"
     ]
    }
   ],
   "source": [
    "%%java\n",
    "import edu.stanford.nlp.ling.*;\n",
    "import edu.stanford.nlp.ie.util.*;\n",
    "import edu.stanford.nlp.pipeline.*;\n",
    "import edu.stanford.nlp.semgraph.*;\n",
    "import edu.stanford.nlp.trees.*;\n",
    "import java.util.*;\n",
    "\n",
    "String text = \"أنا ذاهب إلى السوق. هل تريد أن أحضر لك شيء ما؟ هكذا إذن! نلتقي بعد أن أعود.\";\n",
    "\n",
    "// set up pipeline properties\n",
    "Properties props = new Properties();\n",
    "\n",
    "// set the list of annotators to run\n",
    "props.setProperty(\"annotators\", \"tokenize,ssplit\");\n",
    "props.setProperty(\"tokenize.language\", \"ar\");\n",
    "props.setProperty(\"segment.model\", \"edu/stanford/nlp/models/segmenter/arabic/arabic-segmenter-atb+bn+arztrain.ser.gz\");\n",
    "props.setProperty(\"ssplit.boundaryTokenRegex\", \"[.]|[!?]+|[!\\u061F]+\");\n",
    "\n",
    "// build pipeline\n",
    "StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n",
    "\n",
    "// create a document object\n",
    "CoreDocument document = new CoreDocument(text);\n",
    "\n",
    "// annnotate the document\n",
    "pipeline.annotate(document);\n",
    "\n",
    "for (CoreSentence sentence: document.sentences()){\n",
    "    System.out.println(\"-------------------------------------\");\n",
    "    System.out.println(sentence.text());\n",
    "    for(CoreLabel token: sentence.tokens()){\n",
    "        System.out.print(token.word() +  \", \");\n",
    "    }\n",
    "    System.out.println();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9feefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "أنا ذاهب إلى السوق.\n",
      "انا, ذاهب, الى, السوق, ., \n",
      "-------------------------------------\n",
      "هل تريد أن أحضر لك شيء ما؟\n",
      "هل, تريد, ان, احضر, ل, ك, شيء, ما, ?, \n",
      "-------------------------------------\n",
      "هكذا إذن!\n",
      "هكذا, اذن, !, \n",
      "-------------------------------------\n",
      "نلتقي بعد أن أعود.\n",
      "نلتقي, بعد, ان, اعود, ., \n"
     ]
    }
   ],
   "source": [
    "//Here, we pass the name of the properties file which is located inside the arabic model\n",
    "//under the name : \"StanfordCoreNLP-arabic.properties\" which contaains configurations\n",
    "// build pipeline\n",
    "StanfordCoreNLP pipeline2 = new StanfordCoreNLP(\"arabic\");\n",
    "\n",
    "// create a document object\n",
    "CoreDocument document2 = new CoreDocument(text);\n",
    "\n",
    "// annnotate the document\n",
    "pipeline2.annotate(document2);\n",
    "\n",
    "for (CoreSentence sentence: document2.sentences()){\n",
    "    System.out.println(\"-------------------------------------\");\n",
    "    System.out.println(sentence.text());\n",
    "    for(CoreLabel token: sentence.tokens()){\n",
    "        System.out.print(token.word() + \", \");\n",
    "    }\n",
    "    System.out.println();\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ganymede 1.1.0.20210614 (Java 17)",
   "language": "java",
   "name": "ganymede-1.1.0.20210614-java-17"
  },
  "language_info": {
   "file_extension": ".java",
   "mimetype": "text/x-java",
   "name": "java",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
