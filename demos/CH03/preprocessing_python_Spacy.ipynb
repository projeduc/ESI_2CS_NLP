{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbfa188",
   "metadata": {},
   "source": [
    "# Text preprocessing using Spacy\n",
    "\n",
    "In \"Spacy\", there are a model for each language; it has to be downloaded. To process a language, we have to load its model. The API works with pipes of tasks. By default, all pipes are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c441fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-15 17:42:45.124724: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-15 17:42:45.379354: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 17:42:45.995280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-15 17:42:46.640129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-15 17:42:46.640368: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m450.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/venv/ml/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (59.6.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/venv/ml/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/venv/ml/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/venv/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/venv/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/venv/ml/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/venv/ml/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/venv/ml/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/ml/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# If the model is already on your system, do not activate\n",
    "# This is a model used just for testing, for more accurate models refer to\n",
    "# https://spacy.io/models\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce88b4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 17:43:34.080301: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-15 17:43:34.106882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 17:43:34.549851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-15 17:43:35.059025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-15 17:43:35.059260: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the trained model\n",
    "# For more languages: https://spacy.io/models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Show different tasks\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ab14ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can disable some tasks\n",
    "nlp.select_pipes(disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77fe0c",
   "metadata": {},
   "source": [
    "## I. Sentence tokenization\n",
    "\n",
    "Given a text, get its sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6be86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a text written by Mr. Aries.,\n",
       " It uses U.S. english to illustrate sentence tokenization.]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is a text written by Mr. Aries. It uses U.S. english to illustrate sentence tokenization.'\n",
    "\n",
    "# we can add tasks after disabling them\n",
    "if 'sentencizer' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('sentencizer')\n",
    "\n",
    "# processing a text using all enabled tasks\n",
    "doc = nlp(text)\n",
    "\n",
    "# -------- Method 1 ---------\n",
    "# sents_list = []\n",
    "# for sent in doc.sents:\n",
    "#     sents_list.append(sent.text)\n",
    "\n",
    "# -------- Method 2 ---------\n",
    "# sents_list = [sent.text for sent in doc.sents]\n",
    "\n",
    "# -------- Method 3 ---------\n",
    "sents_list = list(doc.sents)\n",
    "\n",
    "sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298b508",
   "metadata": {},
   "source": [
    "## II. Words tokenization\n",
    "\n",
    "It is automatically executed when calling **nlp(text)**. \n",
    "This is because words are the main component for other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e88cb99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This,\n",
       " is,\n",
       " a,\n",
       " text,\n",
       " written,\n",
       " by,\n",
       " Mr.,\n",
       " Aries,\n",
       " .,\n",
       " It,\n",
       " uses,\n",
       " U.S.,\n",
       " english,\n",
       " to,\n",
       " illustrate,\n",
       " sentence,\n",
       " tokenization,\n",
       " .]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- Method 1 ---------\n",
    "# tokens = []\n",
    "# for word in doc:\n",
    "#     tokens.append(word.text)\n",
    "\n",
    "# -------- Method 2 ---------\n",
    "# tokens = [word.text for word in doc]\n",
    "\n",
    "# -------- Method 3 ---------\n",
    "tokens = list(doc)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a53181",
   "metadata": {},
   "source": [
    "## III. StopWords filtering\n",
    "\n",
    "For each word, there is a boolean attribute **is_stop** which indicates if a word is a stop-word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5d10abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'written',\n",
       " 'Mr.',\n",
       " 'Aries',\n",
       " '.',\n",
       " 'uses',\n",
       " 'U.S.',\n",
       " 'english',\n",
       " 'illustrate',\n",
       " 'sentence',\n",
       " 'tokenization',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = []\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        filtered_tokens.append(word.text)\n",
    "        \n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc240e8a",
   "metadata": {},
   "source": [
    "## IV. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67295246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'sentencizer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('This', 'this'),\n",
       " ('is', 'be'),\n",
       " ('a', 'a'),\n",
       " ('text', 'text'),\n",
       " ('written', 'write'),\n",
       " ('by', 'by'),\n",
       " ('Mr.', 'Mr.'),\n",
       " ('Aries', 'Aries'),\n",
       " ('.', '.'),\n",
       " ('It', 'it'),\n",
       " ('uses', 'use'),\n",
       " ('U.S.', 'U.S.'),\n",
       " ('english', 'english'),\n",
       " ('to', 'to'),\n",
       " ('illustrate', 'illustrate'),\n",
       " ('sentence', 'sentence'),\n",
       " ('tokenization', 'tokenization'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is a text written by Mr. Aries. It uses U.S. english to illustrate sentence tokenization.'\n",
    "\n",
    "\n",
    "nlp.enable_pipe('tagger')\n",
    "nlp.enable_pipe('tok2vec')# apparently it uses this as well\n",
    "nlp.enable_pipe('attribute_ruler')\n",
    "nlp.enable_pipe('lemmatizer') # lemmatizer must use tagger + attribute ruler OR morphologizer\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "lemmas_list = []\n",
    "for word in doc:\n",
    "    lemmas_list.append((word.text, word.lemma_))\n",
    "\n",
    "lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bdbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
