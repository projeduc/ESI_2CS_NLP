% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TeX spellcheck = en_US

\documentclass[11pt, a4paper]{article}
%\usepackage{fullpage}
\usepackage[left=1cm,right=1cm,top=1cm,bottom=2cm]{geometry}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
%\usepackage{indentfirst}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french,english]{babel}
\usepackage{txfonts} 
\usepackage[]{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{longtable}
\usepackage{wasysym}


\usepackage{turnstile}%Induction symbole

\usepackage{tikz}
\usetikzlibrary{arrows, automata}
\usetikzlibrary{decorations.pathmorphing}

\renewcommand{\baselinestretch}{1}

\setlength{\parindent}{0pt} % 24pt


\begin{document}
	
	%\selectlanguage {french}
	%\pagestyle{empty} 
	
	\noindent
	\begin{tabular}{ll}
		\multirow{3}{*}{\includegraphics[width=1.5cm]{../extra/logo/esi.nlp.pdf}} & 
		\'Ecole national Supérieure d'Informatique (ESI), Algiers\\
		& 2CSSID (2022-2023)\\
		& Natural Language Processing (NLP)
	\end{tabular}\\[.25cm]
	\noindent\rule{\textwidth}{2pt}\\[-0.5cm]
	\begin{center}
		{\LARGE \textbf{Tutorial: PoS tagging}}
		\begin{flushright}
			Abdelkrime Aries
		\end{flushright}
	\end{center}\vspace{-0.5cm}
	\noindent\rule{\textwidth}{2pt}


\section{Quizzes}

\subsection{Midterm 2022/2023}

Compare between these PoS tagging methods:
\begin{center}
	\begin{tabular}{|l|l|l|l|l|}
		\cline{2-5}
		\multicolumn{1}{l|}{}
		& HMM & MEMM & RNN & Transformer \\
		\hline
		Statistical model (not neural) & \Square & \Square & \Square & \Square \\ \hline
		Accept heterogeneous features & \Square & \Square & \Square & \Square \\ \hline
		Uses an explicit language model & \Square & \Square & \Square & \Square \\
		\hline
	\end{tabular}
\end{center}

When encoding PoS of a given sentence, Beam search can perform as well as Viterbi in terms of accuracy.
\begin{center}
	\begin{tabular}{|ll|}
		\hline
		\Circle Yes & \Circle No \\
		\hline
	\end{tabular}
\end{center}

\subsection{Midterm 2021/2022}

Given a sentence with "$ T $" words and "$ N $" tags, the complexity of Viterbi algorithm will be $ O(N^2T) $.
If we consider just $ K < N $ for each instant $ t $ (after testing all possible tags with those chosen previously), what would be the complexity? (The viterbi matrix will be [K, T]).

\begin{center}
	\begin{tabular}{|llll|}
		\hline
		\Circle $ O(N^2T) $ & \Circle $ O(KNT) $ & \Circle $ O(K^2T) $ & \Circle $ O(N^2K) $ \\
		\hline
	\end{tabular}
\end{center}

Can this approach ensure optimal solution?
\begin{center}
	\begin{tabular}{|ll|}
		\hline
		\Circle Yes & \Circle No \\
		\hline
	\end{tabular}
\end{center}



\subsection{Master 2022/2023}

Given this French paragraph: \textbf{"Débloquer : dégager ou libérer quelque chose. Le déblocage est son action. Exemple : bloquez et débloquez vos cartes SIM. Le sens inverse est le verbe bloquer qui veut dire immobiliser. Un autre sens de ce mot est : grouper en un bloc. La personne qui, lors d’une grève, bloque l’entrée est appelée bloqueur."}

We want to use MEMMs to apply PoS tagging on this paragraph. 
If we want the maximum of the properties of each word in order to perform the classification, what are the operations to \textbf{avoid} during preprocessing:
\begin{center}
	\begin{tabular}{|lll|}
		\hline
		\Square\  Tokenization & \Square\ Stemming & \Square\ Stop-words filtering \\
		\Square\ Lowercase normalization & \Square\ Use train treebank & \\
		\hline
	\end{tabular}
\end{center}


\subsection{}

Select the right propositions:

\begin{longtable}{|p{.95\textwidth}|}
	\hline 
	\Square\ In PoS using HMM, transitions between states (PoS) are represented as an LM. \\
	\Square\ PoS using HMM can easily integrate heterogeneous features. \\	
	\Square\ MEMM can use those features used by HMM. \\
	\Square\ MEMM is a discriminative model like HMM. \\
	\hline
\end{longtable}


\section{HMM}

\subsection{Master 2023/2024}

We have a treebank (TB) of 3 annotated sentences based on Nominal phrases.
\begin{center}
	\begin{tabular}{|lll|}
		\hline
		[I]\textsubscript{NP} light [the light]\textsubscript{NP} &
		light [the house]\textsubscript{NP} & 
		[You]\textsubscript{NP} lighted [a beautiful torch]\textsubscript{NP} \\
		\hline
	\end{tabular}
\end{center}

Using HMM without smoothing, here (Y=belongs to NP, N=no); you can encode them differently. Calculate: 

\begin{center}
	\begin{tabular}{|l|l|l|l|}
		\hline
		\# labels & p(Y|the) & p(Y Y| the torch) & p(Y Y N| the torch light) \\
		\hline
		&&&\\
		\hline
	\end{tabular}
\end{center}

\subsection{Midterm 2022/2023}

Here is a treebank of 3 sentences:
\begin{center}
	\begin{tabular}{|lll|}
		\hline
		I/P light/V the/D torch/N &
		You/P torch/V the/D house/N &
		I/P house/V the/D light/N \\
		\hline
	\end{tabular}
\end{center}

\begin{itemize}
	\item Using HMM without smoothing, draw a Viterbi table to PoS-tag the expression "You light" (When the probability is 0, write 0; otherwise give more detail.
	\item If we want to detect only nominal phrases, what to do? Annotate the three sentences for this task. 
\end{itemize}


\subsection{Master 2022/2023}

Here is a treebank of 3 sentences:
\begin{center}
	\begin{tabular}{|lll|}
		\hline
		it/P rains/V cats/N and/C dogs/N &
		dogs/N chase/V cats/N &
		it/P rains/V \\
		\hline
	\end{tabular}
\end{center}

Calculate these probabilities using an HMM without smoothing.
\begin{center}
	\begin{tabular}{|l|l|l|l|l|l|}
	\hline
	$ \pi $(N) & P(V|N) & P(N|V) & P(dogs|N) & P(cats|N) & P(chase|V) \\
	\hline
	&&&&&\\
	\hline
	\multicolumn{2}{|l|}{P( N V N | cats chase dogs)} & \multicolumn{4}{|l|}{} \\
	\hline
\end{tabular}
\end{center}

\subsection{Midterm 2021/2022}

Here is a treebank (4 sentences where each word is annotated by its grammatical category):
\begin{center}
	\begin{tabular}{|lll|}
		\hline
		I/PN fish/VB small/AJ fish/NM &&
		I/PN swim/VB like/PR fish/NM \\
		ducks/NM swim/VB &&
		I/PN like/VB big/AJ fish/NM \\
		\hline
	\end{tabular}
\end{center}

Using an HMM without smoothing, calculate the probability \textbf{p(NM VB AJ NM | "ducks like small fish")}.


\subsection{Master 2021/2022}

This is a hidden Markov model for PoS tagging: 

\begin{center}
	\begin{tikzpicture}[
	> = stealth, % arrow head style
	shorten > = 1pt, % don't touch arrow head to node
	auto,
	node distance = 3cm, % distance between nodes
	semithick, % line style
	font=\small
	]
	
	\tikzstyle{every state}=[
	draw = black,
	thick,
	fill = white,
	minimum size = 4mm
	]
	
	\node[circle,draw] (qN) {N};
	\node[align=left,draw] (qNe) [left of=qN] {P("fish"|N)=0.05\\P("river"|N)=0.03};
	\node[circle,draw] (qV) [right of=qN] {V};
	\node[align=left,draw] (qVe) [right of=qV] {P("fish"|V)=0.01\\P("eat"|V)=0.02};
	\node[circle,draw] (qD) [below of=qN] {D};
	\node[align=left,draw] (qDe) [left of=qD] {P("the"|D)=0.3\\P("a"|D)=0.2\\P("an"|D)=0.15};
	\node[circle,draw] (qP) [right of=qD] {P};
	\node[align=left,draw] (qPe) [right of=qP] {P("I"|P)=0.2\\P("he"|P)=0.1\\P("she"|P)=0.1};
	\node[] () [below of=qD, yshift=1.5cm] {$\pi(P, D, V, N) = (0.4, 0.3, 0.1, 0.2)$};
	
	
	\path[->] 	
	(qN) 	edge [loop above] node {0.3} ()
	edge [bend left] node {0.7} (qV)
	(qV) 	edge [loop above] node {0.1} ()
	edge [bend left] node {0.5} (qN)
	edge [bend left] node {0.4} (qD)
	(qD)	edge [bend left] node {1.0} (qN)
	(qP)	edge [bend right] node {1.0} (qV);
	
	\path[dashed] 	
	(qN) 	edge [] node {} (qNe)
	(qV) 	edge [] node {} (qVe)
	(qD) 	edge [] node {} (qDe)
	(qP) 	edge [] node {} (qPe);
	
\end{tikzpicture}
\end{center}

\begin{enumerate}
	\item Calculate these two probabilities: P(V D N | "fish a fish") et P(N D N | "fish a fish")
	\item Given C(N) = 200, C(V) = 100 et C(D)=100, calculate these probabilities using Laplace smoothing: P(D|V), P(D|N) et P(N|D).
	\item Recalculate the probabilities of the first question using this smoothing.
	\item How much labeled expressions "\textbf{V D N}" exist in our training dataset? The determiner "\textbf{D}" does not happen at the end of a sentence. 
	
\end{enumerate}

\end{document}
