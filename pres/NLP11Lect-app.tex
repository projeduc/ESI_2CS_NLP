% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = en_US

\documentclass[xcolor=table]{beamer}

\input{options}

\title[ESI - NLP: 11- Some applications]%
{Natural Language Processing\\Chapter 11\\Some applications} 

\changegraphpath{../img/app/}

\begin{document}
	
\begin{frame}
\frametitle{Natural Language Processing}
\framesubtitle{Some applications: Introduction}

	\begin{itemize}
		\item To present some NLP applications, we will categorize them using some criteria
		\begin{itemize}
			\item Based on support: they process text or speech?
			\item Based on interactivity: they interact with users or no?
			\item Based on output: classes or other text?
		\end{itemize}
		\item Then, we have 4 categories:
		\begin{itemize}
			\item Transformation
			\item Interaction
			\item Classification
			\item Speech
		\end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Natural Language Processing}
\framesubtitle{Some applications: Plan}

\begin{multicols}{2}
%	\small
\tableofcontents
\end{multicols}
\end{frame}

%===================================================================================
\section{Transformation}
%===================================================================================

\begin{frame}
	\frametitle{Some applications}
	\framesubtitle{Transformation}
	\begin{itemize}
		\item \textbf{Input}: text 
		\item \textbf{Output}: text
		\item \textbf{Applications} 
		\begin{itemize}
			\item Machine translation (MT)
			\item Automatic text summarization (ATS)
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Machine translation (MT)}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{Machine translation (MT): Approaches}
	\hgraphpage{MT-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Direct}
	\begin{itemize}
		\item \textbf{Algorithm}:
		\begin{itemize}
			\item The source text $S$ is processed as word series
			\item Each word $S_i$ is replaced by a word $T_i$ to get a target text $T$ en using a bilingual dictionary			
			\item Then, the words are ordered. E.g. \expword{SVO \textrightarrow VSO}, \expword{adj + N \textrightarrow N + Adj}
		\end{itemize}
		\item \textbf{Preconditions}:
		\begin{itemize}
			\item Both languages (source and target) much be close (similar grammatical structures)
			\item A well-designed bilingual dictionary
			\item Tools for morphological processing
		\end{itemize}
		\item \textbf{Level}: Lexical 
		\item E.g. \expword{Systems before 1967: Météo, Weidner, CULT and Systran (first editions)}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Transfer-based}
	\begin{itemize}
		\item \textbf{Algorithm}:
		\begin{itemize}
			\item Find parse tree of $S$.
			\item Translate each word from the source language to target one using a bilingual dictionary.
			\item Apply rules to transform the parse tree from the source language to the target one.
			\item Generate the target text $T$ from the resulted parse tree.
		\end{itemize}
		\item \textbf{Preconditions}:
		\begin{itemize}
			\item Both languages grammars
			\item Transfer rules are defined manually or learned from a corpus
			\item A well-designed bilingual dictionary
			\item Tools for morphological processing and parsing (source language)
			\item Tool for text generation based on parse trees (target language)
		\end{itemize}
		\item \textbf{Level}: Syntactic
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Transfer-based (Example of transfer rules)}

	\begin{figure}
		\centering
		\hgraphpage[\textwidth]{MT-transfer-exp.pdf}
		\caption{Example of syntactic transfer rules \cite{06-quah}}
	\end{figure}

\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Transfer-based (Example: Apertium \cite{11-forcada-al})}
	
	\begin{itemize}
		\item \url{https://www.apertium.org/}
	\end{itemize}

	\hgraphpage{MT-apertium-arch.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Interlingua-based}
	
	\begin{figure}
		\centering
		\hgraphpage[.9\textwidth]{MT-Interlingua.pdf}
		\caption{Example of Interlingua-based MT model \cite{06-quah}}
	\end{figure}

\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Interlingua-based (Description)}
	\begin{itemize}
		\item \textbf{Algorithm}:
		\begin{itemize}
			\item Parse the source text $S$
			\item Use a dictionary which maps source language words to Interlingua concepts 
			\item Transform the parse tree (source language) to Interlingua
			\item Transform Interlingua to parse tree in target language
			\item Use a dictionary which maps Interlingua concepts to target language words 
			\item Generate target text $T$
		\end{itemize}
		\item \textbf{Preconditions}:
		\begin{itemize}
			\item Tools for parsing (both syntactic and semantic)
			\item An Interlingua
			\item Interlingua concepts dictionary
		\end{itemize}
		\item \textbf{Level}: syntactic/semantic
	\end{itemize}
\end{frame}


\begin{frame}[fragile]
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Interlingua-based (Example KANT \cite{98-czuba-al})}
\begin{columns}
\begin{column}{0.5\textwidth}
\bfseries\fontsize{4}{5}\selectfont
\begin{verbatim}
(*A-REMAIN  ; action rep for ’remain’
   (FORM FINITE)
   (TENSE PAST)
   (MOOD DECLARATIVE)
   (PUNCTUATION PERIOD)
   (IMPERSONAL -) ; passive + expletive subject
   (ARGUMENT-CLASS THEME+PREDICATE) ; predicate argument structure
   (Q-MODIFIER ; PP semrole (generic)
      (*K-DURING ; PP interlingua
         (POSITION FINAL) ; clue for translation
            (OBJECT ; PP object semrole
               (*O-TIME ; object rep for ’time’
                  (UNIT -)
                  (NUMBER SINGULAR)
                  (REFERENCE DEFINITE)
                  (DISTANCE NEAR)
                  (PERSON THIRD)))))
   (THEME ; object semrole
      (*O-DEFAULT-RATE ; object rep for ’default rate’
         (PERSON THIRD)
         (UNIT -)
         (NUMBER SINGULAR)
         (REFERENCE DEFINITE)))
   (PREDICATE ; adjective phrase semrole
      (*P-CLOSE ; property rep for ’closer’
         (DEGREE POSITIVE)
         (Q-MODIFIER
            (*K-TO
               (OBJECT
                  (*O-ZERO
                     (UNIT -)
                     (NUMBER SINGULAR)
                     (REFERENCE NO-REFERENCE)
                     (PERSON THIRD))))))))
\end{verbatim}
\end{column}
\begin{column}{0.5\textwidth}
	\begin{figure}
		\caption{Sentence "\textit{The default rate remained close to zero during this time.}" representation using KANT Interlingua \cite{98-czuba-al}}
	\end{figure}
\end{column}
\end{columns}


\end{frame}

\begin{frame}[fragile]
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Interlingua-based (Example: KANTOO system \cite{00-nyberg-al})}
	
	\hgraphpage{MT-kantoo-arch.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Statistical}
	\[
	p(T|S) = \frac{p(T) p(S|T)}{p(S)} \propto \underbrace{p(T)}_\text{Coherence} \underbrace{p(S|T)}_\text{Fidelity}
	\]
	\[\hat{T} = \arg\max_{T} p(T) p(S|T)\]
	\[\]
	\begin{itemize}
		\item $p(T)$ is a language model trained on target language
		\begin{itemize}
			\item Must have a target language's corpus
			\item $p(T) = \prod_{j=1}^m p(t_j|t_{j-N+1}\ldots t_{j-1})$
		\end{itemize}
		
		\item $p(S|T)$ is a translation model 
		\begin{itemize}
			\item Must have an aligned corpus source/target
			\item Words alignment problem (not the same order)
			\item Size difference between source/target
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Statistical (Alignment)}
	
	\[p(S|T) = \sum_{A} p(S, A | T)\]
	
%	\[A^* = \arg\max_A p(S, A | T)\]
	\[p(S, A | T) = \prod_{i=1}^{n} p(S_i, A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m})\]
	
	\[p(S_i, A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
	
	\[ \text{Number of possible alignments } = (m + 1)^n\]
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Statistical (IBM1 alignment)}
	\begin{itemize}
		\item IBM 1 model (Uniform distribution of words)
	\end{itemize}

	\[p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = \frac{1}{m+1}\]
	
	\[p(S|T) = \frac{1}{(m+1)^n} \sum_{A} \prod_{i=1}^{n} p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Statistical (IBM2 alignment)}
	\begin{itemize}
		\item IBM 2 model
		\item Alignments probabilities is only based on the current position $i$, the current word $A_i$, the source text size $n$ and the target text size $m$
		\item This probability is estimated by comparing the correct alignment  with the rest
	\end{itemize}
	
	\[p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = p(A_i | i, n, m)\]
	
	\[p(S|T) = \sum_{A} \prod_{i=1}^{n} p(A_i | i, n, m) p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Statistical (HMM alignment)}
	\begin{itemize}
		\item HMM model
		\item The alignment probability of a word $A_i$ is based on the previous word's alignment $A_{i-1}$
	\end{itemize}
	
	\[p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = p(A_i | A_{i-1}, m)\]
	
	\[p(S|T) = \sum_{A} \prod_{i=1}^{n} p(A_i | A_{i-1}, m) p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Examples-based}
	\begin{itemize}
		\item The same as statistical approach
		\item Instead of words, we can find segments
		\item A segment's probability (set of consecutive words) is calculated compared to another
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Examples-based (Example: Moses \cite{07-koehn-al})}
	\begin{itemize}
		\item \url{http://statmt.org/moses/}
		\item Segment translation table $\phi(S|T)$: segment $S$, equivalent segment $T$ and a probability.
		\item Language model on target language $LM$.
		\item Distortion model $ D(T, S) $: each segments' reorganization of a sentence is worth a cost.
		\item Words penalty $W(T)$: so that a translation is not long or short.
		\item To estimate $\hat{T}$, \keyword{Beam search} is used.
	\end{itemize}

	\[p(T|S) = \phi(S|T)^{poids_{\phi}} \times LM^{poids_{LM}} \times D(T, S)^{poids_{D}} \times W(T)^{poids_{W}}\]
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: NN-based}
	\begin{itemize}
		\item Structure
		\begin{itemize}
			\item Model: encoder-decoder (seq2seq)
			\item \optword{Encoder}: encodes a sentence of source language $S$. The result is a representation of the context as a vector
			\item \optword{Decoder}: decodes a context vector to a sentence of target language $T$
		\end{itemize}
	
		\item Formal model
		
		\[ p(T|S) = p(t_1|S) p(t_2|S, t_1) p(t_3|S, t_1, t_2)\ldots p(t_m|S, t_1\ldots t_{m-1}) \]
		
		\[\hat{T} = \arg\max_{T} \prod_{i=1}^{m} p(t_i | S, t_1\ldots t_{i-1})\]
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: NN-based (Example: Google \cite{16-wu-al})}
	\begin{itemize}
		\item \url{https://translate.google.com/}
	\end{itemize}
	\begin{center}
		\hgraphpage[.9\textwidth]{MT-googlet.pdf}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: NN-based (Example: OpenNMT \cite{17-klein-al})}
	\begin{itemize}
		\item \url{https://opennmt.net/}
	\end{itemize}
	\hgraphpage{MT-opennmt.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{MT: Some humor}
	
	\begin{center}
		\hgraphpage[0.65\textwidth]{humor/humor-translation1.jpg}
		\hgraphpage[0.25\textwidth]{humor/humor-translation2.jpeg}
	\end{center}
	
\end{frame}

\subsection{Automatic text summarization (ATS)}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{Automatic text summarization (ATS): Summary categories}
	\hgraphpage{ATS-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Approaches}
	\begin{columns}
		\begin{column}{0.4\textwidth}
			\begin{block}{\scriptsize\bfseries\cite{12-nenkova-mckeown}}
				\begin{itemize}
					\item Topic representation:
					topic words,
					frequencies,
					latent semantic analysis,
					Bayesian topic models,
					clustering

					\item Indicators representation: 
					graph-based, 
					ML
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.3\textwidth}
			\begin{block}{\scriptsize\bfseries\cite{12-lloret-palomar}}
				\begin{itemize}
					\item statistical 
					\item graph-based
					\item discourse-based
					\item ML-based
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.28\textwidth}
			\begin{block}{\scriptsize\bfseries\cite{19-aries-al}}
				\begin{itemize}
					\item statistical
					\item graph-based
					\item linguistic 
					\item ML-based
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Statistical approach}
	
	\begin{itemize}
		\item \optword{Words frequency} 
		
		\hspace{.5cm}E.g. $Score_\text{TF-IDF}(s_i) = \sqrt{\sum\limits_{w_{ik} \in s_i} (\text{TF-IDF}(w_{ik}))^2}$
		
		\item \optword{Sentence (or words) Position}
		
		\hspace{.5cm}E.g. $ Score_\text{position}(s_i) = \max (\frac{1}{i}, \frac{1}{|D| - i + 1}) $
		
		\item \optword{Sentence length}
		
		\hspace{.5cm}E.g. $ Score_\text{length}(s_i) = \left\lbrace 
		\begin{array}{lll}
		0 & si & (L_i \geq L_{min}) \\
		\frac{L_i - L_{min}}{L_{min}} & sinon & \\
		\end{array}
		\right. $
		
		\item \optword{Title and subtitle words}
		
		\hspace{.5cm}E.g. $ Score_{title}(s_i) = \frac{\sum_{e \in T \bigcap s_i}{\frac{tf(e)}{tf(e)+1}}}
		{\sum_{e \in T}{\frac{tf(e)}{tf(e)+1}}} $
		
		\item \optword{Centroid}, \optword{Frequent itemsets}, \optword{LSA}, ...
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Statistical approach (Example: \cite{13-aries-al} (1))}
	
	\hgraphpage{ATS-tcc-arch.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Statistical approach (Example: \cite{13-aries-al} (2))}
	
	\begin{itemize}
		\item A text can discuss several subjects, as well a single sentence
		\begin{itemize}
			\item Clustering sentences using their similarities and a clustering threshold ($Th$)
		\end{itemize}
		\item A sentence is pertinent if it can represent the maximum of subjects (clusters)
		\[ Score(s_i , c_j , f_k ) = 1 + \sum_{\phi \in s_i} {P(f_k=\phi | s_i \in c_j)} \]
		\[ Score(s_i , \bigcap_{j} c_j , F) =  %\propto 
		\prod_{j} \prod_{k} Score(s_i , c_j , f_k ) \]
		$ s $: sentence, $ c $: cluster, $ f $: feature, $ F $: set of features, $ \phi $: observation of $ f $.
		\item $f$: TF (Uni, Bi); Pos (interval of 10); Len (all words, filtered)
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Statistical approach (Example: \cite{15-oufaida-al} (1))}
	\begin{itemize}
		\item Words representation: pre-trained embeddings (Polyglot)
		\item Clustering to extract text's sub-subjects
		\begin{itemize}
			\item Find $w_j \in S_2$; the word most similar to the word $w_i$
			
			\[Match(w_i | S_2) = \arg\max_{w_j \in S_2} sim(Rep(w_i), Rep(w_j))\]
			
			\item Similarity between $S_1$ and $S_2$
			
			\[Sim(S_1, S_2) = \frac{\sum_{w_i \in S_1} Match(w_i | S_2) + \sum_{w_j \in S_2} Match(w_j | S_1)}{|S_1| + |S_2|}\]
			
			\item Apply a clustering algorithm on sentences/sentences similarity matrix
		\end{itemize}
	
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Statistical approach (Example: \cite{15-oufaida-al} (2))}

	\begin{itemize}
		\item Terms score based on mRMR
		\begin{itemize}
			\item Mutual information (X, Y: term/sentence vectors) 
			
			\begin{center}
				$I(X, Y) = \sum\limits_{x \in X} \sum\limits_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$
			\end{center}
			
			\item Terms relevance (H: cluster/sentence vector)
			\begin{center}
				$Relevance(T_i) = I(T_i, H)$
			\end{center}
			
			\item Terms redundancy (S: all sentences)
			
			\begin{center}
				$Redundancy(T_i) = \frac{1}{|S|} \sum\limits_{j \in S} I(T_i, T_j)$
			\end{center}
			
			\item Terms final score: terms mRMR vector
			
			\begin{center}
				$MID \equiv \max_{t \in T} Relevance(t) - Redundancy(t)$
			
			$MIQ \equiv \max_{t \in T} Relevance(t) / Redundancy(t)$
			\end{center}
		\end{itemize}
		
		\item A sentence score is its similarity with mRMR vector
		
		\item When selecting a sentence, mRMR vector is decreased by the selected terms weights
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Graph-based approach}
	
	\begin{itemize}
		\item \optword{Graph properties}
		\begin{itemize}
			\item Bushy paths 
			
			\hspace{.5cm}$Score_{\#arcs}(s_i) = |\{ s_j : a(s_i, s_j) \in A / s_j \in S, s_i \neq s_j \}|$
			
			\item Aggregate similarity
			
			\hspace{.5cm}$Score_{aggregate}(s_i) = \sum\limits_{(s_i, s_j) \in E} sim(s_i, s_j)$
		\end{itemize}
		\item \optword{Iterative methods}
		\begin{itemize}
			\item Update node scores based on its neighbors
			\item Stopping: a state of equilibrium (we can no longer update)
			\item E.g. TextRank \cite{04-mihalcea-tarau}
			
			$WS(V_i) = ( 1 - d) + d * \sum\limits_{V_j \in In(V_i)} \frac{w_{ji}}{\sum\limits_{V_k \in Out(V_j)} w_{jk}} WS(V_j)$
			
			$w_{ij} = \frac{|\{w_k \text{ / } w_k \in S_i \text{ and } w_k \in S_j\}|}{\log(|S_i|) + \log(|S_j|)}$
			
			$ d $: damping factor (in general, around $ 0.85 $)
		\end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Graph-based approach (Example: \cite{21-aries-al} (1))}
	
	\begin{center}
		\vgraphpage{ATS-gc-archi.pdf}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Graph-based approach (Example: \cite{21-aries-al} (2))}
	
	\vspace{-6pt}
	\begin{itemize}
		\item Graph simplification
		
		\hspace{.5cm}$weak\_node(v_i) = ( \sum_{(v_i, v_j) \in E} w_{ij} < \frac{1}{MImpN(v_i)} )$ 
		
		\hspace{.5cm}$weak\_arc(v_i, v_j) = ( w_{ij} < \frac{Threshold}{MImpN(v_i)})$
		
		\item Statistical score of a sentence
		
		\hspace{.5cm}$ Score(s_i/ sim) = sim(s_i, C\backslash s_i) $
		$Score(s_i/ tfisf) = \sqrt{\sum\limits_{w_{ik} \in s_i} (tfisf(w_{ik}))^2}$
		
		\hspace{.5cm}$Score(s_i/ size) = \frac{1}{|s_i|}$
		$Score(s_i/ pos) = \max (\frac{1}{i}, \frac{1}{|D| - i + 1})$
		
		\hspace{.5cm}$SSF(s_i/ F) = \prod_{f_i \in F} score(s_i/f_i)$
		
		\item Graph-based score of a sentence
		
		\hspace{.5cm}E.g. $GC1(s_i) = SSF(s_i) + \sum\limits_{(s_i, s_j) \in E} sim(s_i, s_j) * SSF(s_j)$
		
		\item Extraction
		
		\hspace{.5cm}E.g. $ next_{e4}  =  \arg\min\limits_i (iord\ gc(s_i) + ord\ sim(last_{e4}, i))$ 
		
		\hspace{3cm}$ \text{ where } (last_{e4}, s_i) \in E $
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Linguistic approach}
	
	\begin{itemize}
		\item \optword{Subject words}: a list of subject-pertinent words, such as ``significant", ``impossible", etc.
		\begin{itemize}
			\item E.g. \cite{69-edmundson}: Bonus (positively pertinent words), Stigma (negatively pertinent words)
			
			$Score_{cue}(s_i) = \sum_{w \in s_i}{cue(w)}
			\text{ where }
			cue(w) = \left\lbrace 
			\begin{array}{ll}
			b > 0 & \text{if } (w \in Bonus) \\
			\delta < 0 & \text{if } (w \in Stigma) \\
			0 & \text{otherwise} 
			\end{array} 
			\right. $
		\end{itemize}
		\item \optword{Indicators}: Structures that imply that the sentence containing them has something important about the subject
		\begin{itemize}
			\item E.g. \expword{the principal aim of this paper is to investigate ...}
		\end{itemize}
		\item \optword{Co-reference}: use of anaphoras or semantic representations (E.g. Wordnet)
		\item \optword{Rhetorical structure}: use of rhetorical structure to score sentences or phrases
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Linguistic approach (Example: \cite{81-paice})}
	
	\begin{itemize}
		\item Using manually prepared templates
		\item\ [x]: up to x words can be between he current word and the next
		\item +y: update the score by y
		\item ?: the current word is optional
	\end{itemize}

	\begin{figure}[!ht]
		\begin{center}
			\hgraphpage[.7\textwidth]{ATS-paice-template.pdf}
			\caption{Example of a simplified template \cite{81-paice}.}
			\label{fig:paice-template}
		\end{center}
	\end{figure}
	
\end{frame}


\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: ML-based}
	
	\begin{itemize}
		\item \optword{Features-based}
		\begin{itemize}
			\item Tuning: tune hyper-parameters such as statistical scores' weights
			\item Classification: decide if a text unit (sentence) belongs to the summary or not
		\end{itemize}
		\item \optword{Bayesian topic models}
		\begin{itemize}
			\item Identify principal concepts from documents et links between them to create a hierarchy
		\end{itemize}
		\item \optword{Deep learning}
		\begin{itemize}
			\item Tuning or Classification
			\item Words generation (a form of classification)
		\end{itemize}
		\item \optword{Reinforcement learning}
		\begin{itemize}
			\item Use actions and rewards to rain a system to generate summaries
		\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: ML-based (Example: \cite{2020-aries})}
	
	\begin{center}
		\vgraphpage{ATS-ml2es-archi.pdf}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: ML-based (Example: \cite{06-daumeiii-marcu})}
	
	\begin{minipage}{.6\textwidth}
		\begin{itemize}
			\item $D$: a set of $K$ documents
			\item $Q$: a set of $J$ queries
			\item $P^G$: a language model trained on general English 
			\item $P^Q$: a language model trained on requests
			\item $P^D$: a language model trained on documents
		\end{itemize}
	\end{minipage}
	\begin{minipage}{.38\textwidth}
		\hgraphpage{ATS-btm-daumeiii.pdf}
	\end{minipage}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: ML-based (Example: \cite{15-rush-al})}
	
	\begin{center}
		\hgraphpage[.35\textwidth]{ATS-2015-rush-al-arch.pdf}
		\hgraphpage[.35\textwidth]{ATS-2015-rush-al-exp.pdf}
	\end{center}
	
	\begin{itemize}
		\item[(a)] A decoder which generates the next word $y_{i+1}$ of the summary based on the input text $x$ and the words already generated $y_c \equiv [y_{i-c+1},\ldots, y_i]$
		\item[(b)] An encoder with attention
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: ML-based (Example: \cite{18-narayan-al})}
	
	\hgraphpage{ATS-narayan-al.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Transformation}
	\framesubtitle{ATS: Some humor}
	
	\begin{center}
		\hgraphpage[0.6\textwidth]{humor/humor-summarize1.jpg}
		\hgraphpage[0.3\textwidth]{humor/humor-summarize2.jpg}
	\end{center}
	
\end{frame}

%===================================================================================
\section{Interaction}
%===================================================================================

\begin{frame}
	\frametitle{Some applications}
	\framesubtitle{Interaction}
	\begin{itemize}
		\item \textbf{Input}: a request (text)
		\item \textbf{Output}: a response (text)
		\item \textbf{Applications} 
		\begin{itemize}
			\item Questions/Answering
			\item Dialogue systems
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Question-Answering (QA)}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Question-Answering (QA)}

	\hgraphpage{QA-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: IR-based}
	
	\begin{figure}
		\hgraphpage{QA-IR.pdf}
		\caption{Architecture of a question/answering system \cite{2019-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: IR-based (Question processing)}
	
	\begin{itemize}
		\item Request formulation
		\begin{itemize}
			\item Words tokenization
			\item Stop words filtering
			\item Stemmer  
		\end{itemize}
	    \item Answer type detection
	    \begin{itemize}
	    	\item Person? Place? Organization? ...
	    	\item Using taxonomies such as Wordnet  
	    \end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: IR-based (search)}
	
	\begin{itemize}
		\item Documents retrieval
		\begin{itemize}
			\item Using keywords and documents index
			\item Return the most scored documents
			\item Split these documents to passages (paragraphs or sentences)
		\end{itemize}
		\item Passages retrieval
		\begin{itemize}
			\item Apply NER on passages
			\item Filter passages which do not contain the answer type
			\item ML algorithms can be used for filtering passages by scoring them
			\item Features: number of named entities of the answer's type, number of terms in the question, the longest most similar sequence to the question, document order, etc.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: IR-based (Answer extraction)}
	
	\begin{itemize}
		\item \optword{Features-based ML}
		\begin{itemize}
			\item Patterns: \expword{\textless AP\textgreater such as \textless QP\textgreater; }
			\item Matched answer type
			\item Number of matched question's keywords 
			\item Novelty factor: at least one word does not exist in the question
			\item Punctuation
			\item ...
		\end{itemize}
		\item \optword{NN}
		\begin{itemize}
			\item Reading/Understanding task
			\item For each word, calculate the probability of being the start of the answer and the end of the answer using the question.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: IR-based (Extraction using Bi-LSTM \cite{2017-chen-al})}
	
	\vskip-6pt
	\begin{figure}
		\vgraphpage[0.8\textheight]{QA-bilstm-exp.pdf}\vskip-3pt
		\caption{Extracting answers in DrQA system \cite{2019-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: IR-based (extraction using BERT \cite{2018-devlin-al})}
	
	\begin{figure}
		\centering
		\hgraphpage[.5\textwidth]{QA-bert-exp.pdf}
		\caption{Answer span extraction using BERT \cite{2019-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: Knowledge-based}
	
	\begin{itemize}
		\item \optword{Graph-based}
		\begin{itemize}
			\item Apply Entity linking
			\item Determine the searched relation; E.g. \expword{Birth\_place}
			\item If there are many answer relations: calculate he similarity between the question and the answer
			\item Example, \expword{using embeddings}
		\end{itemize}
		\item \optword{Semantic-parsing-based}
		\begin{itemize}
			\item Semantic parsing of the question
			\item This will generate a structured for of the question: lambda calculus, SQL, SPARQL, etc.
			\item This forme est is used as a database request
			\item Retrieve the answer from a knowledge base
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: Knowledge-based (Example)}
	
	\begin{exampleblock}{Example of logical forms \cite{2020-jurafsky-martin}}
		\centering\tiny\bfseries
		\begin{tabular}{p{0.5\textwidth}p{0.45\textwidth}}
			\hline\hline
			Question & Logical form \\
			\hline
			What Wilayas border Jijel? 
			& $\lambda x.wilaya(x) \wedge border(x, Jijel)$ \\
			
			What is the largest Wilaya? 
			& $\arg\max(\lambda x.wilaya(x), \lambda x.area(x))$ \\
			
			I'd like to book a flight from Jijel to Algiers. 
			& {\color{blue}SELECT DISTINCT} f1.flight id
			
			{\color{blue}FROM} vol v1, airport  a1,
			
			wilaya w1, airport\_service a2, wilaya w2
			
			{\color{blue}WHERE} v1.depart=a1.airport\_code
			
			{\color{blue}AND} a1.wilaya\_code=w1.wilaya\_code
			
			{\color{blue}AND} w1.wilaya\_name= '{\color{red}Jijel}'
			
			{\color{blue}AND} v1.destination=a2.airport\_code
			
			{\color{blue}AND} a2.wilaya\_code=w2.wilaya\_code
			
			{\color{blue}AND} w2.wilaya\_name= '{\color{red}Algiers}' \\
			
			
			How many people survived the sinking of Titanic ? 
			& (count (!fb:event.disaster.survivors
			
			fb:en.sinking of the titanic))\\
			
			How many yards longer was Johnson's longest touchdown compared to his shortest touchdown of the first quarter?
			& 
			ARITHMETIC diff( SELECT num( ARGMAX(
			
			SELECT ) ) SELECT num( ARGMIN( FILTER(
			
			SELECT ) ) ) )\\
			\hline\hline
		\end{tabular}
	\end{exampleblock}
	
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: LM-based}
	
	\begin{itemize}
		\item Use a pretrained language model
		\item Fine-tune the model on question/answering task
	\end{itemize}

	\begin{figure}
		\centering
		\hgraphpage[.6\textwidth]{qa-t5.pdf}
		\caption{T5 is pretrained on filling absent text, then fine-tuned for question/answering without any additional information or context. \cite{2020-roberts-al}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{QA: Some humor}
	
	\begin{columns}
		\begin{column}{0.3\textwidth}
			\hgraphpage{humor/humor-QR1.jpg}
			
			\hgraphpage{humor/humor-QR3.jpeg}
		\end{column}
		\begin{column}{0.4\textwidth}
			\hgraphpage{humor/humor-QR2.jpeg}
		\end{column}
	\end{columns}

\end{frame}

\subsection{Dialogue systems}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems}
	\hgraphpage{DS-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Task-oriented - Frame-based (Frame)}
	
	\begin{itemize}
		\item Frame: a structure containing filling slots and predefined questions for each slot.
		\item Only empty slots' questions are asked.
		\item There is a possibility to fill other frames' slots. 
		E.g. \expword{The slot RESERVATION\_DATE of the frame HOTEL\_RESERVATION can be filled from the slot ARRIVAL\_DATE of the frame FLIGHT\_RESERVATION}.
		\item Apply \keyword{Intention detection} to detect which frame to use.
	\end{itemize}

	\begin{exampleblock}{Example of flight frame \cite{2020-jurafsky-martin}}
		\centering\tiny\bfseries
		\begin{tabular}{lll}
			\hline\hline
			Slot & Type & Question \\
			\hline
			ORIGIN CITY & city & ``From what city are you leaving?" \\
			DESTINATION CITY & city & ``Where are you going?" \\
			DEPARTURE TIME & time & ``When would you like to leave?" \\
			DEPARTURE DATE & date & ``What day would you like to leave?" \\
			ARRIVAL TIME & time & ``When do you want to arrive?" \\
			ARRIVAL DATE & date & ``What day would you like to arrive?" \\
			\hline\hline
		\end{tabular}
	\end{exampleblock}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Task-oriented - Frame-based (Multiple answers)}
	
		\centering\footnotesize
		\begin{tabular}{lll}
			SHOW & \textrightarrow & show me \textbar\ I want \textbar\ can-I see \\
			DEPARTTIME & \textrightarrow & (after \textbar\ around \textbar\ before) HOUR \textbar\\
			&  & morning \textbar\ afternoon \textbar\ evening\\
			DEPARTDATE & \textrightarrow & on (Saturday \textbar\ ... \textbar\ Friday)\\
			HOUR & \textrightarrow & (one \textbar\ two \textbar\ ... \textbar\ twelve) (am \textbar\ pm) \\
			FLIGHTS & \textrightarrow & (a) flight \textbar\ flights \\
			ORIGIN & \textrightarrow & from WILAYA \\
			DESTINATION & \textrightarrow & to WILAYA \\
			WILAYA & \textrightarrow & Adrar \textbar\ ... \textbar\ El Meniaa \\
		\end{tabular}
		
		\hgraphpage[.8\textwidth]{DS-frame-parse-exp.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Task-oriented - Dialogue-State}
	
	\begin{figure}
		\centering
		\hgraphpage{DS-dialog-arch.pdf}
		\caption{Architecture of a system using dialogue-state \cite{2016-williams-al}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Task-oriented - Dialogue-State (Dialogue acts)}
	
	\vspace{-4pt}
	\begin{figure}
		\centering\tiny\bfseries
		\begin{tabular}{llll}
			\hline
			Tag & Sys & User & Description \\
			\hline
			HELLO (a = x, b = y, ...) & \CheckedBox & \CheckedBox & Open a dialogue and give info a = x, b = y, ... \\
			INFORM(a = x, b = y, ...) & \CheckedBox & \CheckedBox & Give info a = x, b = y, ... \\
			REQUEST(a, b = x, ...) & \CheckedBox & \CheckedBox & Request value for a given b = x, ... \\
			REQALTS(a = x, ...) & \XBox & \CheckedBox & Request alternative with a = x, ... \\
			CONFIRM(a = x, b = y, ...) & \CheckedBox & \CheckedBox & Explicitly confirm a = x, b = y, ... \\
			CONFREQ (a = x, ..., d) & \CheckedBox & \XBox & Implicitly confirm a = x, ... and request value of d \\
			SELECT(a = x, a = y) & \CheckedBox & \XBox & Implicitly confirm a = x, ... and request value of d \\
			AFFIRM(a = x, b = y, ...) & \CheckedBox & \CheckedBox & Affirm and give further info a = x, b = y, ... \\
			NEGATE(a = x) & \XBox & \CheckedBox & Negate and give corrected a = x \\
			DENY(a = x) & \XBox & \CheckedBox & Deny that a = x \\
			BYE () & \CheckedBox & \CheckedBox & Close a dialogue \\
			\hline
		\end{tabular}
	
		\begin{tabular}{p{0.5\textwidth}p{0.4\textwidth}}
			\hline
			Utterance & Dialogue act\\
			\hline
			U: Hi, I am looking for somewhere to eat.  & hello(task=find, type=restaurant) \\
			S: You are looking for a restaurant. What type of food do you like?  & confreq(type=restaurant, food) \\
			U: I'd like an Italian somewhere near the museum. & inform(food=Italian, near=museum)\\
			S: ``Roma" is a nice Italian restaurant near the museum.  & inform(name=``Roma", type=restaurant, food=Italian, near=museum) \\
			U: Is it reasonably priced? & confirm(pricerange=moderate) \\
			S: Yes, ``Roma" is in the moderate price range.  & affirm(name=``Roma", pricerange=moderate) \\
			U: What is the phone number? & request(phone) \\
			S: The number of ``Roma" is 385456. & inform(name=``Roma", phone=``385456") \\
			U: Ok, thank you goodbye. & bye() \\
			\hline
		\end{tabular}
		
		\caption{\vspace{-1cm}Restaurant recommendation using HIS \cite{2010-young-al}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Task-oriented - Dialogue-State (Slots filling)}
	
	\begin{itemize}
		\item Classify the sentence by intention, domain and slot
		\item Extract information to fill slots. E.g. \expword{Using sequences labeling}
	\end{itemize}
	
	\begin{figure}
		\centering
		\hgraphpage[.4\textwidth]{DS-dialog-fill-exp.pdf}
		\caption{Architecture for slots filling using BERT \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Task-oriented - Dialogue-State (Other components)}
	
	\begin{itemize}
		\item Dialogue state tracker
		\begin{itemize}
			\item Determine the current state of the frame (the fillers of each slot).
			\item Determine the user's most recent dialogue act.
		\end{itemize}
	
		\item Dialogue Policy
		\begin{itemize}
			\item Determine the next action $A_i$ 
			\[\hat{A}_i = \arg\max_{A_i \in A} P(A_i | Frame_{i-1}, A_{i-1}, U_{i-1})\]
		\end{itemize}
	
		\item NLG in dialogue-state 
		\begin{itemize}
			\item Generate a text from the dialogue act
			\item We can train an encoder/decoder model (Seq2Seq, Transformers)
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Chatbot - Rule-based (Example: ELIZA \cite{1966-Weizenbaum})}
	
	\begin{itemize}
		\item The most known rule-based chatbot is \keyword{ELIZA}: psychological counseling.
		\item Rules: a list of tuples pattern/transform 
		
		\expword{\small [(.*) YOU (.*) ME]\textsubscript{[Pattern]} \textrightarrow\ [WHAT MAKES YOU THINK I \$2 YOU?]\textsubscript{[Transform]}}
		
		\expword{You hate me \textrightarrow\ WHAT MAKES YOU THINK I HATE YOU?}
		
		\item Each rule's template is linked to a list of words. 
		The most scored word word in the sentence will trigger multiple templates.
		
		\item Among them, the most similar template to the sentence will be used.
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Chatbot - IR-based}
	
	\begin{itemize}
		\item A corpus of conversations $C$.
		\item The user's text will be considered as a request $q$.
		\item The response $r \in C$ which is most similar to $q$ is retrieved.
		\[\text{Response}(q, C) = \arg\max_{r \in C} \frac{q . r}{|q| |r|}\]
		\item To calculate the similarity, TF-IDF can be used.
		\item Also, $q$ and $r$ can be encoded using embeddings
		\[h_q = BERT_Q(q)[CLS],\; h_r = BERT_R(r)[CLS]\]
		\[\text{Response}(q, C) = \arg\max_{r \in C} q . r\]
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Chatbot - NLG-based}
	
	\begin{itemize}
		\item Use an encoder/decoder (seq2seq or Transformers)
		\[ \hat{r}_i = \arg\max_{w \in V} p(w| q, r_1, \ldots, r_{t-1}) \]
		
		\item The encoder can take a more long context (see the figure)
		
		\item We can use language models, such as GPT, to train them on conversations.
	\end{itemize}
	
	\begin{figure}
		\centering
		\hgraphpage[.7\textwidth]{DS-chatbot-encdec-exp.pdf}
		\caption{Example of a chatbot based on encoder/decoder \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}


\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Chatbot - NLG-based (Examples)}
	
	\begin{itemize}
		\item ChatGPT is a sibling project to instructGPT \cite{2022-ouyang-al}
		\begin{itemize}
			\item based on \keyword{GPT-3.5} which was fine-tuned on conversations provided by human AI trainers playing both sides: the user and an AI assistant. 
			\item GPT is a Transformers-decoder pretrained on next word prediction
			\item In this case, having the user's request, the task is to predict next words which are the system's response
			\item Using Reinforcement Learning from Human Feedback (RLHF): the reward is calculated from manually scored conversations.
			\item It was optimized using Proximal Policy Optimization \cite{2017-schulman-al}
		\end{itemize}
	\item Similar open source projects
	\begin{itemize}
		\item \url{https://github.com/lucidrains/PaLM-rlhf-pytorch}
		\item \url{https://github.com/CarperAI/trlx}
	\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{Some applications: Interaction}
	\framesubtitle{Dialogue systems: Some humor}
	
	\begin{center}
		\hgraphpage[0.5\textwidth]{humor/humor-chatbots1.jpeg}
		\hgraphpage[0.4\textwidth]{humor/humor-chatbots2.jpeg}
	\end{center}
\end{frame}


%===================================================================================
\section{Classification}
%===================================================================================

\begin{frame}
	\frametitle{Some applications}
	\framesubtitle{Classification}
	
	\begin{itemize}
		\item \textbf{Input}: a text
		\item \textbf{Output}: a class
		\item \textbf{Applications} 
		\begin{itemize}
			\item Spam filtering
			\item Language identification
			\item Readability
			\item Sentiment analysis
			\item Humor detection
			\item Author detection
			\item ...
		\end{itemize}
	\end{itemize}

\end{frame}

\subsection{Sentiment analysis}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis}
	\hgraphpage{SA-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis: Knowledge-based}
	
	\begin{itemize}
		\item Identify sentiment-indicator words
		\begin{itemize}
			\item In general, adjectives and adverbs are good indicators
			\item \optword{Dictionary-based}: using dictionaries such as Wordnet to augment the list of indicators (synonyms, antonyms, etc.) 
			\item \optword{Corpus-based}: using co-occurrence with the original list's words to augment it 
		\end{itemize}
		\item Assign scores to these words (or labels)
		\item Calculate total score based on words
		\begin{itemize}
			\item \textbf{Granularity}: sentences, paragraphs, documents, etc. 
			\item The negation can be a problem
			\item We can use structures; for example \keyword{RST}
			\item E.g. \expword{Give a score of -1 to negative words and +1 to positive ones. Then, sum words polarities to get a sentence polarity}
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis: ML-based}
	
	\begin{itemize}
		\item \optword{Features-based}
		\begin{itemize}
			\item Word presence and its frequency 
			\item PoS
			\item Opinion words and phrases
			\item Negation
		\end{itemize}
		\item \optword{Embeddings-based}
		\begin{itemize}
			\item Using words embeddings to learn the output class
			\item Using BERT (Transfer learning)
			\begin{itemize}
				\item Using the output \keyword{[CLS]} to estimate classes
				\item Using \keyword{[CLS]} with MLP
				\item Using all hidden states with configurations such as CNN, RNN, etc.
			\end{itemize}
		\end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis: hybrid (example: \cite{18-bettiche-al} (1))}
	
	\begin{figure}
		\centering
		\hgraphpage[.6\textwidth]{SA-bettiche-al.pdf}
		\caption{Hybrid architecture proposed by \cite{18-bettiche-al} to detect polarity on Algerian dialect}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis: hybrid (example: \cite{18-bettiche-al} (2)) }
	
	\begin{itemize}
		\item Vocabulary enrichment
		\begin{itemize}
			\item Similar words detection (by appointing a representative)
			
			$Ratio = 1 - Levenstein(w_1, w_2)/(|w_1|+|w_2|)$
			
			E.g. \expword{Ratio(kolach, kollach) = 92\%, Ratio(kolach, khlasse) = 38\%.}
			
			\item Calculate semantic orientation of a word $w$
			
			
			$SO(w) = \sum_{w_p \in V_p} PMI(w, w_p) - \sum_{w_n \in V_n} PMI(w, w_n)$
			
			$PMI (w_1, w_2) = \log \frac{p(w_1, w_2)}{p(w_1)*p(w_2)}$
		\end{itemize}
		\item Polarity representation
		\begin{itemize}
			\item $polarity(w) = +1 \text{ IF } SO(w) > 0; -1 \text{ IF } SO(w) < 0$
			\item $polarity(w) = SO(w)$
		\end{itemize}
		\item Rule-based version: $polarity(msg) = Signe \sum_{w \in msg} polarity(w)$
		\item ML-based version
		\begin{itemize}
			\item Features: words polarities
			\item Input words selction: using $PMI$ similarity
			\item Many ML algorithms are tested: NB, DT, SVM, etc.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis: hybrid (example: \cite{18-guellil-al})}
	
	\begin{figure}
		\centering
		\hgraphpage[.45\textwidth]{SA-guellil-al.pdf}
		\caption{Hybrid architecture proposed by \cite{18-guellil-al} to analyze messages in Arabizi}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis: Some humor}
	
	\begin{center}
		\vgraphpage{humor/humor-sentiment.jpg}
	\end{center}
	
\end{frame}

\subsection{Readability}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Readability}
	\hgraphpage{Readability-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Readability: Formula (Flesch-Kincaid Grade Level)}
	\[
	206.835 - 1.015 (\frac{\text{\slshape total words}}{\text{\slshape total sentences}})
	- 84.6 (\frac{\text{\slshape total syllables}}{\text{\slshape total words}})
	\]
	
	\begin{center}
			\footnotesize
	    \begin{tblr}{
	    		colspec = {p{.15\textwidth}lp{.5\textwidth}},
	    		row{odd} = {lightblue},
	    		row{even} = {lightyellow},
	    		row{1} = {darkblue},
	    	} 
			\bfseries\textcolor{white}{Score} && \bfseries\textcolor{white}{Difficulty}\\
			90-100 && Very easy to read (5th grade). \\
			80-90 && Easy to read (6th grade).\\
			70-80 && Fairly easy to read (7th grade).\\
			60-70 && Plain English (8th \& 9th grade). \\
			50-60 && Fairly difficult to read (10th o 12th grade). \\
			30-50 && Difficult to read (college). \\
			10-30 && Very difficult to read (college graduate). \\
			0-10 && Extremely difficult to read (professional). \\
		\end{tblr}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Readability: Formula (Dale–Chall readability formula)}
	\[
	0.1579 (\frac{\text{\slshape difficult words}}{\text{\slshape total words}})
	+ 0.0496 (\frac{\text{\slshape total words}}{\text{\slshape total sentences}})
	\]
	
	\begin{center}
		\footnotesize
		\begin{tblr}{
				colspec = {p{.15\textwidth}lp{.5\textwidth}},
				row{odd} = {lightblue},
				row{even} = {lightyellow},
				row{1} = {darkblue},
			} 
			\bfseries\textcolor{white}{Score} && \bfseries\textcolor{white}{Difficulty}\\
			\textless= 4.9 && Grade 4 and below. \\
			5-5.9 && Grades 5–6. \\
			6-6.9 && Grades 7–8.\\
			7-7.9 && Grades 9–10. \\
			8-8.9 && Grades 11–12. \\
			9-9.9 && Grades 13–15 (college). \\
			10+ && Grades 16 and above. \\
		\end{tblr}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Readability: Formula (OSMAN \cite{2016-elhaj-rayson})}
	
	\begin{align*}
		Osman = & 200.791 - 1.015 \times (\frac{\text{\slshape total words}}{\text{\slshape total sentences}}) - 24.181 \times \\
		& \\
		&  (\frac{\text{\slshape difficult words + syllables + complex words + Faseeh words}}{\text{\slshape total words}})
	\end{align*}

	\vfill
	
	\begin{itemize}
		\item Difficult words: more than 5 characters without diacritics
		\item Complex words: more than 4 syllables
		\item Faseeh words: complex words with letters \<|', y', w', _d, .z> or ending with \<wn, wA>
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Readability: ML-based}
	
	\begin{figure}
		\centering
		\hgraphpage{Readability-ML.pdf}
		\caption{Pipeline of reading difficulty estimation using ML \cite{2014-collins}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Classification}
	\framesubtitle{Sentiment analysis: Some humor}
	
	\begin{center}
		\vgraphpage{humor/humor-readability1.jpg}
	\end{center}
	
\end{frame}

%===================================================================================
\section{Speech}
%===================================================================================

\begin{frame}
	\frametitle{Some applications}
	\framesubtitle{Speech}
	\begin{itemize}
		\item Speech recognition
		\begin{itemize}
			\item \textbf{Input}: speech
			\item \textbf{Output}: text
			\item \textbf{More specific application} : speech to text (STT)
		\end{itemize}
		\item Speech synthesis
		\begin{itemize}
			\item \textbf{Input}: text
			\item \textbf{Output}: speech
			\item \textbf{More specific application}: text to speech (TTS)
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Speech recognition}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Classification}
	\hgraphpage{ASR-classif.pdf}
\end{frame}


\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Features extraction (Sampling and Quantification)}
	\begin{itemize}
		\item Convert the analog representations into a digital signal
		\item Each vector represent signal information encoded using a short time window
		\item \optword{Sampling}
		\begin{itemize}
			\item Must take 2 samples per cycle (to capture both signal parts: positive and negative)
			\item Human speech frequencies are less than 10 KHz (sampling rate = 20 KHz)
			\item In telephony, the frequency is 4KHz (sampling rate = 8 KHz)
		\end{itemize}
		\item \optword{Quantification}
		\begin{itemize}
			\item Amplitudes of samples are stored as integers
			\item Either 8 bits (-128 to 127) or 16 bits (-32768 to 32767)
			\item The value of a sample at a time index $n$ is represented as $x[n]$
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Features extraction (Windowing (1))}
	
	\begin{itemize}
		\item Using a window to capture a part of the phoneme
		\item This part is called: \keyword{Frame}
		\item Two parameters: Window size and Frame stride (shift, offset)
	\end{itemize}

	\begin{figure}
		\centering
		\hgraphpage[.5\textwidth]{ASR-windowing-exp.pdf}
		\caption{Example of Windowing \cite{2020-jurafsky-martin}}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Features extraction (Windowing (2))}
	\begin{itemize}
		\item To extract a frame $y[n]$, initial signal $s[n]$ is multiplied by a window $w[n]$ 
		\[y[n] = w[n] s[n]\]
		\item The most simple is rectangular window 
		\[w[n] = \begin{cases}
			1 & \text{if } 0 \le n \le L-1 \\
			0 & \text{else}\\
		\end{cases}\]
		\item Problems with Fourier analysis (cutting the signal at the boundaries)
		\item The solution is to use Hamming window
		\[w[n] = \begin{cases}
			0.54 - 0.46 \cos (\frac{2\pi n}{L}) & \text{if } 0 \le n \le L-1 \\
			0 & \text{else}\\
		\end{cases}\]
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Features extraction (Windowing: example)}
	
	\begin{figure}
		\centering
		\hgraphpage[.5\textwidth]{ASR-windowing2-exp.pdf}
		\caption{Example of Rectangular and Hamming windows \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Features extraction (DFT)}
	\begin{itemize}
		\item \optword{Discrete Fourier Transform}
		\item How much energy a signal contains in different frequency bands
		\[X[k] = \sum\limits_{n=0}^{N-1} x[n] e^{-j\frac{2\pi}{N} k n}\]
	\end{itemize}
	\begin{figure}
		\centering
		\hgraphpage[.65\textwidth]{ASR-DFT-exp.pdf}
		\caption{(a) a signal portion of the vowel [iy] (b) its DFT \cite{2020-jurafsky-martin}}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Features extraction (Mel filter)}
	\begin{itemize}
		\item \optword{Mel filter}
		\item Human hearing is not equally sensitive to all frequency bands
		\item Collect the energies at each frequency based on the Mel scale
		\[mel(f) = 1127 \ln (1 + \frac{f}{700})\]
	\end{itemize}
	\begin{figure}
		\centering
		\hgraphpage[.65\textwidth]{ASR-mel-exp.pdf}
		\caption{Example of Mel filter \cite{2020-jurafsky-martin}}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Recognition (1)}
	\begin{figure}
		\centering
		\hgraphpage[.75\textwidth]{ASR-rec-exp.pdf}
		\caption{Example of recognition using an encoder-decoder \cite{2020-jurafsky-martin}}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Recognition (1)}
	\begin{itemize}
		\item $p(y_1, \ldots, y_n) = \prod\limits_{i=1}^n p(y_i| y_1, \ldots, y_{i-1}, X)$
		
		\item $\hat{y}_i = \arg\max_{c \in V} p(c| y_1, \ldots, y_{i-1}, X)$
		
		\item Clearly, this is a language model
		\item But, data can be insufficient to learn a good language generation
		\item Solution: Integrate a separate language model
		
		\item $score(Y|X) = \frac{1}{|Y|_{car}} \log p(Y|X) + \lambda \log p_{LM}(Y)$
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech recognition: Some humor}
		\begin{center}
			\vgraphpage{humor/humor-ASR.jpg}
		\end{center}
\end{frame}

\subsection{Speech synthesis}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech synthesis}
	\hgraphpage{tts-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech synthesis: Architecture}
	\begin{figure}
		\centering
		\hgraphpage[.53\textwidth]{TTS-arch.pdf}
		\caption{Architecture of a voice synthesis system \cite{2017-Hinterleitner}}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Some applications: Speech}
	\framesubtitle{Speech synthesis: Some humor}
	\begin{center}
		\vgraphpage{humor/humor-TTS.jpg}
	\end{center}
\end{frame}


\insertbibliography{NLP11}{*}

\end{document}

