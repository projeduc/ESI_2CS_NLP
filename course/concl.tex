% !TEX TS-program = xelatex
% !TeX program = xelatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

%=====================================================================
\ifx\wholebook\relax\else
	\documentclass{KodeBook}
	\input{calls}
	\begin{document}
		\mainmatter
	
\fi
%=====================================================================

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}



% structure du cours
Ce support du cours est le fruit d'un travail de plus de 3 ans. 
Il essaye de présenter les différents concepts du domaine TALN d'une manière simple, concise et droit au but.
Dans ce cas, seulement les tâches les plus importantes ou souvent présentées par d'autres universités sont inclues.
Les tâches restantes sont similaires et donc le fait de les présenter est considéré comme redondance.
Chaque chapitre est réservé à une tâche sauf le premier et le dernier.
Le premier chapitre présente le domaine du TALN afin de motiver ce cours. 
Il prépare les étudiants à ce qu'ils vont étudier dans ce module en essayant de présenter quelques concepts linguistiques.
Les tâches simples sont présentées dans les chapitres suivants afin que l'étudiant ait des blocs pour construire des applications plus avancées.
Ces tâches sont présentées par l'ordre du niveau de traitement d'un langage.
Nous pouvons remarqué que le niveau pragmatique n'est pas inclus.
En réalité, ce niveau est partagé entre le niveau sémantique, où nous devons utiliser des ressources comme FrameNet afin de comprendre une phrase, et le niveau discours où la coréférence et la cohérence sont résolues en se basant sur le contexte.
Le dernier chapitre présente quelques applications avancées dans le domaine.


Ce cours ne présente qu'une petite partie du domaine de TALN.
Afin de se documenter plus sur un sujet particulier, il faut lire des articles ou des livres dédiés à ce même sujet.
Les deux livres les plus cités ici \cite{2020-jurafsky-martin,2018-eisenstein} sont un bon point de départ car ils sont utilisés par de nombreuses universités américaines.
Il existe plusieurs cours similaires à celui-ci :
\begin{itemize}
	
	\item Natural Language Processing S21 (à jours).
	Carnegie Mellon University. 
	\url{http://demo.clab.cs.cmu.edu/NLP/} [visité le 2022-05-19].
	
	\item CS388: Natural Language Processing (2018). 
	University of Texas. 
	\url{https://www.cs.utexas.edu/~mooney/cs388/} [visité le 2022-05-19].
	
	\item CSE 517: Natural Language Processing ().
	University of Washington.
	\url{https://courses.cs.washington.edu/courses/cse517/} [visité le 2022-05-19].
	
	\item CS 294-5: Statistical Natural Language Processing (2005). 
	University of Berkely. 
	\url{https://people.eecs.berkeley.edu/~klein/cs294-5/index.html} [visité le 2022-05-19].
	
	\item CS224n: Natural Language Processing with Deep Learning (à jours).
	Stanford university.
	\url{https://web.stanford.edu/class/cs224n/} [visité le 2022-05-19].
	
	\item CS 5340/6340: Natural Language Processing (à jours). 
	University of Utah.
	\url{https://my.eng.utah.edu/~cs5340/schedule.html} [visité le 2022-05-19].
	
\end{itemize}



%=====================================================================
\ifx\wholebook\relax\else
% \cleardoublepage
% \bibliographystyle{../use/ESIbib}
% \bibliography{../bib/RATstat}
	\end{document}
\fi
%=====================================================================
