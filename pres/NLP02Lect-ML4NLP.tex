% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = en_US

\documentclass[xcolor=table]{beamer}

\usepackage{../extra/beamer/karimnlp}

\input{options}

\subtitle[ML for NLP]{Chapter 02\\Machine learning for NLP} 

\changegraphpath{../img/ml4nlp/}

\begin{document}
	

\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{ML: Example (Simple Perceptron for binary logical functions)}
	
	\begin{minipage}{0.30\textwidth} 
		\hgraphpage{perceptron.pdf}
	\end{minipage}
	%
	\begin{minipage}{0.59\textwidth}
		\scriptsize
		\begin{itemize}
			\item $ z^{(i)} = x_1^{(i)} w_1 + x_2^{(i)} w_2 + b $
			\item $ \hat{y}^{(i)} = \phi(z^{(i)}) = \begin{cases}
				1 & \text{if } z^{(i)} \ge 0 \\
				0 & \text{otherwise}
			\end{cases} $
			\item $ w_j = w_j + \nabla w_j$ where $ \nabla w_j = \frac{1}{M}\sum_{i=0}^{M} (y^{(i)} - \hat{y}^{(i)}) * x_j^{(i)} $
		\end{itemize}
	\end{minipage}
	
	\vfill
	
	\begin{exampleblock}{Training a Perceptron for AND function}
		\begin{minipage}{0.2\textwidth} 
			\scriptsize
			\begin{tabular}{|c|c|c|}
				\hline
				x\textsubscript{1} & x\textsubscript{2} & y \\
				\hline
				0 & 0 & 0  \\
				\hline
				0 & 1 & 0 \\
				\hline
				1 & 0 & 0 \\
				\hline
				1 & 1 & 1 \\
				\hline
			\end{tabular}
		\end{minipage}
		%
		\begin{minipage}{0.79\textwidth}
			\scriptsize
			\begin{itemize}
				\item Initially, $ W = [w_1, w_2, b] = [0, 0, 0] $.
				
				\item It1: $ Z = \hat{Y} = [0, 0, 0, 0] $, 
				$ \nabla W = [\frac{1}{4}, \frac{1}{4}, \frac{1}{4}] $, 
				$ W = [\frac{1}{4}, \frac{1}{4}, \frac{1}{4}] $.
				
				\item It2: $ Z = [\frac{1}{4}, \frac{2}{4}, \frac{2}{4}, \frac{3}{4}] $, 
				$ \hat{Y} = [1, 1, 1, 1] $, 
				$ \nabla W = [\frac{-1}{4}, \frac{-1}{4}, \frac{-3}{4}] $,
				$ W = [0, 0, \frac{-1}{2}] $.
				
				\item It3: $ Z = [\frac{-1}{2}, \frac{-1}{2}, \frac{-1}{2}, \frac{-1}{2}] $, 
				$ \hat{Y} = [1, 1, 1, 1] $, 
				$ \nabla W = [\frac{1}{4}, \frac{1}{4}, \frac{1}{4}] $,
				$ W = [\frac{1}{4}, \frac{1}{4}, \frac{-1}{4}] $.
				
				\item It4: $ Z = [\frac{-1}{4}, 0, 0, \frac{1}{4}] $, 
				$ \hat{Y} = Y = [0, 0, 0, 1] $ (STOP)
			\end{itemize}
		\end{minipage}
	\end{exampleblock}
	
\end{frame}
	
\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{ML: Algorithms and abbreviations}
	
	\begin{itemize}
		\item Traditional ML algorithms
		\begin{itemize}
			\item \optword{NB}: Naive Bayes
			\item \optword{LR}: Logistic regression
			\item \optword{SVM}: Support-Vector Machine
			\item \optword{HMM}: Hidden Markov Model
			\item \optword{MEMM}: Maximum-Entropy Markov Model
			\item \optword{CRF}: Conditional Random Field
		\end{itemize}
		\item \optword{FFNN}: Feed Forward Neural Network
		\begin{itemize}
			\item \optword{MLP}: Multi-Layer Perceptron
			\item \optword{CNN}: Convolutional Neural Network
		\end{itemize} 
		\item \optword{RNN}: Recurrent Neural Network
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{ML: Types of models}
	
	\scriptsize
	\begin{tblr}{
			colspec = {p{.12\textwidth}lp{.37\textwidth}lp{.38\textwidth}},
			row{odd} = {lightblue},
			row{1} = {darkblue, fg=white, font=\bfseries, valign=m, halign=c},
			column{2,4}={white},
			column{1}={bg=darkblue, fg=white, font=\bfseries, valign=m},
			row{even} = {white},
			cell{1}{1}={white},
			colsep=3pt,
			rowsep=3pt,
			stretch = 0,
		}
		
		&& Generative && Discriminative \\
		
		&&&&\\
		
		Text && NB && LR, SVM, MLP, RNN, CNN \\
		
		&&&&\\
		
		Sequence && HMM  && MEMM, CRF, RNN \\
		
	\end{tblr}
	
	\vfill
	
	\begin{itemize}
		\item \optword{Generative model}: a model that learns to generate features given a class:
		\[\hat{Y} = \arg\max_k P(Y_k) P(X | Y_k)\]
		
		\item \optword{Discriminative model}: a model that learns to estimate a class given some features: 
		\[\hat{Y} = \arg\max_k P(Y_k | X)\]
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{NLP: Text classification}
	
	\begin{itemize}
		\item Entire text classifying:
		\begin{itemize}
			\item \optword{Sentiment analysis}: text (such as tweets) classifying into positive, negative or neutral sentiment.
			\item \optword{Spam detection}: message classifying as spam ou not spam.
			\item \optword{Figurative language detection}: text classifying as metaphor, irony, etc.
		\end{itemize}
		\item One sentence at the time classifying:
		\begin{itemize}
			\item \optword{Automatic text summarization}: sentence classifying as belonging to the summary or not.
			\item \optword{Textual implication}: checking whether a sentence implies another.
		\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{NLP: Sequence classification}
	
	\begin{itemize}
		\item Given a sentence, each word must be classified separately.
		\item Example, \expword{Assign a part of speech to each word}.
		\item The word, as well as its class, can depend on the preceding (or/and following) words.
		\item \textbf{Problem}: how to classify a sequence of words that have the same (non-atomic) class?
		\item Example, \expword{[\textit{Abdelkrime Aries}]\textsubscript{\bfseries PERSON} is a teacher at the [\textit{Ecole nationale Supérieure d'Informatique}]\textsubscript{\bfseries INSTITUTE}}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{NLP: Sequence classification (IOB representation)}
	
	\begin{itemize}
		\item \keyword{IOB}: Inside, Outside, Beginning
		\item For each class \textbf{CLS}, two different classes (\textbf{B-CLS} and \textbf{I-CLS}) are created.
		\item \optword{B-CLS} marks the beginning of the class.
		\item \optword{I-CLS} marks the continuation of the class.
		\item \optword{O} means "no class".
		\item Example, \expword{Abdelkrime/\textsubscript{\bfseries B-PERSON} Aries/\textsubscript{\bfseries I-PERSON}  is/\textsubscript{\bfseries O} a/\textsubscript{\bfseries O} teacher/\textsubscript{\bfseries O} at/\textsubscript{\bfseries O} the/\textsubscript{\bfseries O} Ecole/\textsubscript{\bfseries B-INSTITUTE} nationale/\textsubscript{\bfseries I-INSTITUTE} Supérieure/\textsubscript{\bfseries I-INSTITUTE} d'/\textsubscript{\bfseries I-INSTITUTE} Informatique/\textsubscript{\bfseries I-INSTITUTE}}.
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{NLP: Text generation}
	
	\begin{itemize}
		\item It is simply the task of finding the next word given the past ones
	\end{itemize}
	
	\[\hat{w}_i = \arg\max\limits_{w_i} P(w_i | \hat{w}_{1}, \cdots, \hat{w}_{i-1})\]
	
\end{frame}


\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{\insertshortsubtitle: Plan}
	
	\begin{multicols}{2}
		\small
		\tableofcontents
	\end{multicols}
\end{frame}



%===================================================================================
\section{Traditional ML and MLP-only architectures}
%===================================================================================

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	\begin{itemize}
		\item For text classification, we have to ensure a fixed sized encoding for all texts (vector)
		\item For sequence classification/tagging
		\begin{itemize}
			\item HMMs are traditionally used
			\item MLPs and other algorithms can be used in a Markovian fashion 
		\end{itemize}
		\item For text generation
		\begin{itemize}
			\item Bigrams are traditionally used
			\item MLPs and other algorithms can be used in a Markovian fashion 
		\end{itemize}
	\end{itemize}
	
\end{frame}


\subsection{Text classification}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Examples}
	
	\vgraphpage{text_class_trad_exp.pdf}
	
\end{frame}

\subsection{Sequence classification}


\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Markov model}
	
	\begin{minipage}{.64\textwidth}
			\begin{itemize}
					\item Markov hypothesis
					\item $ P(q_i = a | q_1, \ldots, q_{i-1}) \approx P(q_i = a | q_{i-1}) $
					\item $Q = \{q_1, q_2, \ldots, q_n\}$: stats.
					\item $A = \begin{bmatrix}%
							a_{11} & a_{12} & \ldots & a_{1n} \\
							a_{21} & a_{22} & \ldots & a_{2n} \\
							\vdots & \vdots & \ddots & \vdots \\
							a_{n1} & a_{n2} & \ldots & a_{nn} \\
						\end{bmatrix}$: transition probability matrix.
					\item $\sum_j a_{ij} = 1,\, \forall i$.
				\end{itemize}
		\end{minipage}
	\begin{minipage}{.35\textwidth}
			\hspace*{-1cm}
			\begin{tikzpicture}[
					> = stealth, % arrow head style
					shorten > = 1pt, % don't touch arrow head to node
					auto,
					node distance = 2cm, % distance between nodes
					semithick, % line style
					font=\tiny\bfseries
					]
					
					\node[circle,draw] (qC) {C};
					\node[circle,draw] (qH) [below left of=qC] {H};
					\node[circle,draw] (qW) [below right of=qC] {W};
					
					\path[->] 	
					(qC) 	edge [loop above] node {0.8} ()
					edge [] node {0.1} (qH)
					edge [bend left] node {0.1} (qW)
					(qH) 	edge [loop left] node {0.6} ()
					edge [bend left] node {0.1} (qC)
					edge [] node {0.3} (qW)
					(qW)	edge [loop right] node {0.6} ()
					edge [bend left] node {0.3} (qH)
					edge [] node {0.1} (qC);
				\end{tikzpicture}
		\end{minipage}
	
	\begin{itemize}
			\item $\pi = [\pi_1, \pi_2, \ldots, \pi_n ]$: states initial probability distribution.
			\item $\sum_i \pi_i = 1$.
			\vspace{-6pt}\item E.g. \expword{Calculate $P(H\, W\, C\, C)$ where $\pi = [\overbrace{0.1}^C, \overbrace{0.7}^H, \overbrace{0.2}^W]$}.
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Hidden Markov model}
	\begin{minipage}{.54\textwidth}
			\begin{itemize}
					\item $Q = \{q_1, q_2, \ldots, q_n\}$: states.
					\item $A$: transition probability matrix where $\sum_j a_{ij} = 1,\, \forall i$.
					\item $O = o_1 o_2 \ldots o_T$: sequence of observed events (words).
					\item $B = b_i(o_t)$: observation probabilities (\keyword{emission probabilities}), each represents the probability of generating an observation $o_t$ in a state $q_i$.
				\end{itemize}
		\end{minipage}
	\begin{minipage}{.45\textwidth}
			\begin{tikzpicture}[
					> = stealth, % arrow head style
					shorten > = 1pt, % don't touch arrow head to node
					auto,
					node distance = 1.5cm, % distance between nodes
					semithick, % line style
					font=\bfseries\fontsize{4}{4}\selectfont
					]
					
					\node[circle,draw] (q2) {MD};
					\node[align=center,draw] (q2e) [left of=q2] {B2\\ \\P(``will"|MD)\\...\\P(``the"|MD)\\...\\P(``back"|MD)};
					\node[circle,draw] (q3) [right of=q2] {NN};
					\node[align=center,draw] (q3e) [right of=q3] {B3\\ \\P(``will"|NN)\\...\\P(``the"|NN)\\...\\P(``back"|NN)};
					\node[circle,draw] (q1) [below of=q2] {VB};
					\node[align=center,draw] (q1e) [left of=q1] {B1\\ \\P(``will"|VB)\\...\\P(``the"|VB)\\...\\P(``back"|VB)};
					
					\path[->] 	
					(q1) 	edge [loop below] node {a11} ()
					edge [bend left] node {a12} (q2)
					edge [] node {a13} (q3)
					(q2) 	edge [loop above] node {a22} ()
					edge [bend left] node {a21} (q1)
					edge [bend right] node {a23} (q3)
					(q3)	edge [loop above] node {a33} ()
					edge [bend left] node {a31} (q1)
					edge [bend right] node {a32} (q2);
					
					\path[dashed] 	
					(q1) 	edge [] node {} (q1e)
					(q2) 	edge [] node {} (q2e)
					(q3) 	edge [] node {} (q3e);
					
				\end{tikzpicture}
		\end{minipage}
	
	\begin{itemize}
			\item $\pi = [\pi_1, \pi_2, \ldots, \pi_n ]$: states initial probability distribution.
			\item $\sum_i \pi_i = 1$.
		\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: HMM Training}
	
	\begin{itemize}
			\item States are tags (categories) $t_i$.
			\item Let's define \keyword{START} as sentences' start tag.
			\item The observations $o_i$ are the words $w_i$.
			\item \keyword{C}: Number of occurrences in the training corpus.
		\end{itemize}
	
	\begin{block}{HMM training}
			\[
			\text{Transition probabilities: } P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_i)}{C(t_{i-1})} 
			\]\[
			\text{Emission probabilities: } P(w_i | t_i) = \frac{C(t_i, w_i)}{C(t_i)}
			\]\[
			\text{Initial distribution: } \pi_i = P(t_i | START) = \frac{C(START, t_i)}{C(START)}
			\]
		\end{block}
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: HMM Labeling (tag decoding)}
	
	\begin{itemize}
			\item Given
			\begin{itemize}
					\item a Markov model $\lambda = (A, B)$,
					\item an observations' sequence (words): $w = w_1 w_2 \ldots w_n$
				\end{itemize}
			\item Estimate a labels' sequence $\hat{t} = \hat{t}_1 \hat{t}_2 \ldots \hat{t}_n$
		\end{itemize}
	
	\begin{block}{Labels' decoding using HMM}
			\[
			\hat{t} = \arg\max\limits_t P(t | w) = \arg\max\limits_t \frac{P(w|t) P(t)}{P(w)} = \arg\max\limits_t P(w|t) P(t)%\text{ tel que } t = t_1 t_2 \ldots t_n
			\]
			
			\[ 
			P(w | t) \approx \prod\limits_{i=1}^n P(w_i|t_i) 
			\hskip2cm
			P(t) \approx \prod\limits_{i=1}^n P(t_i|t_{i-1}) 
			\]
			
			\[
			\hat{t} = \arg\max\limits_t \prod\limits_{i=1}^n P(w_i|t_i) P(t_i|t_{i-1})
			\]
		\end{block}
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: HMM \& Viterbi algorithm}
	
	\begin{block}{Viterbi}
			\scriptsize
			\begin{algorithm}[H]
		%				\vskip-1em
					\KwData{$w = w_1 \ldots w_T$, HMM $\lambda = (A, B)$ with $N$ states}
					\KwResult{$best\_path$, $prob\_path$}
					
					Create a matrix $viterbi[N, T]$\;
					
					\lForEach{state $ s = 1 \ldots N$}{
							$viterbi[s, 1] = \pi_s * b_s(w_1);\, backpointer[s, 1] = 0$
						}
					
					\ForEach{tag $ t = 2 \ldots T$}{
							\ForEach{state $ s = 1 \ldots N$}{
									$viterbi[s, t] = \max\limits_{s'=1}^N viterbi[s', t-1] * a_{s',s} * b_s(w_t)$\;
									$backpointer[s, t] = \arg\max\limits_{s'=1}^N viterbi[s', t-1] * a_{s',s} * b_s(w_t)$\;
								}
						}
					
					$prob\_path = \max\limits_{s=1}^N viterbi[s, T];\, pointer\_path = \arg\max\limits_{s=1}^N viterbi[s, T]$\;
					
					$best\_path$ is the path starting from $pointer\_path$ and following $backpointer$
					
					\Return $best\_path$, $prob\_path$\;
					
				\end{algorithm}
		\end{block}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: MEMM/MLP}
	
	\begin{itemize}
			\item Let $x_{n}^{m} = x_n x_{n+1} \ldots x_{m-1} x_m$
			\item Given 
			\begin{itemize}
					\item an observations sequence (words): $w = w_1 w_2 \ldots w_n$
					\item a set of features $f$ defined on the sequences (such as: majuscule word)
				\end{itemize}
			\item Estimate tags sequence $\hat{t} = \hat{t}_1 \hat{t}_2 \ldots \hat{t}_n$
		\end{itemize}
	
	\begin{block}{Labels decoding using MEMM}
			\[
			\hat{t} = \arg\max\limits_t P(t | w) = \arg\max\limits_t \prod\limits_{i}  P(t_i | w_i, t_{i-1})
			\]
			
			\[
			\hat{t} = \arg\max\limits_t \prod\limits_{i}  
			\frac{exp\left(\sum_j \theta_j f_j(t_i, w_{i-l}^{i+l}, t_{i-k}^{i-1})\right)}%
			{\sum_{t' \in tags} exp\left(\sum_j \theta_j f_j(t'_i, w_{i-l}^{i+l}, t_{i-k}^{i-1})\right)}
			\]
		\end{block}
	
\end{frame}


\subsection{Text generation}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item Using NGrams with a size N
		\item The probability of each word $ w_i $ is calculated based on $N-1$ past words
		\item The most probable word is chosen
		\[\hat{w}_i = \arg\max\limits_{w_i} P(w_i | w_{i-N+1}, \cdots, w_{i-1})\]
		\item The probabilities of NGrams are estimated based on a training corpus
		\item \keyword{This will be seen in "Language models" chapter}
		\item MLPs can be used in the same fashion
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: MLP-based text generation (example)}
	
	\[\hat{w}_i = \arg\max\limits_{w_i} P(w_i | \hat{w}_{i-N+1}, \cdots, \hat{w}_{i-1})\]
	
	\begin{center}
		\vgraphpage[0.7\textheight]{text_gen_MLP_exp.pdf}
	\end{center}
	
\end{frame}

\subsection{Embedding Layer}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
%	\[\hat{w}_i = \arg\max\limits_{w_i} P(w_i | \hat{w}_{i-N+1}, \cdots, \hat{w}_{i-1})\]
	
%	\begin{center}
%		\vgraphpage[0.7\textheight]{text_emb_exp.pdf}
%	\end{center}
	\hgraphpage[\textwidth]{text_emb_exp.pdf}
	
\end{frame}


%===================================================================================
\section{CNN-based architectures}
%===================================================================================

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	\begin{itemize}
		\item For text classification, 
		\begin{itemize}
			\item we have to ensure a fixed sized encoding for all texts (matrix)
			\item an MLP is used as output 
		\end{itemize}
		\item For sequence classification/tagging
		\begin{itemize}
			\item Like classification, but past tags can be used as a second channel 
		\end{itemize}
		\item For text generation
		\begin{itemize}
			\item Like classification; but instead of a class, the next word is estimated
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{Text classification}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
			\item For each word of the text, a vector representation must be defined (vector of \keyword{N} elements).
			\item Representations are concatenated vertically (\keyword{M} words).
			\item This will result in a matrix of \keyword{M X N} elements.
			\item A one-dimensional convolution (dimension = \keyword{K}) is applied; i.e. a \keyword{K X N} dimension filter.
			\item Several filters can be used to learn several representations.
			\item Pooling can also be used.
			\item A \keyword{MLP} layer (or several) is used at the end to estimate the class.
			\item \textbf{Problem}: Technically, we cannot define a neural network with a variable number of words.
			\item \textbf{Solution}: \textcolor{yellow!30}{A maximum number must be defined; if the number of words is lower, padding is added; otherwise, extra words are deleted.}
		\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Examples}
	
	\vgraphpage{text_class_CNN_exp.pdf}
	
\end{frame}

\subsection{Sequence classification}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\vgraphpage{text_seq_CNN_exp.pdf}
	
\end{frame}


\subsection{Text generation}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\vgraphpage{text_gen_CNN_exp.pdf}
	
\end{frame}


%===================================================================================
\section{RNN-based architectures}
%===================================================================================

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	\begin{itemize}
		\item For text classification, 
		\begin{itemize}
			\item the last state is used as input to an MLP
			\item the MLP estimates the class
		\end{itemize}
		\item For sequence classification/tagging
		\begin{itemize}
			\item An MLP is used to encode each word's state
		\end{itemize}
		\item For text generation
		\begin{itemize}
			\item The next word is estimated until reaching "end of sentence" tag
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{Text classification}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item For each word of the text, a vector representation must be defined (vector of \keyword{N} elements).
		\item The RNN network creates M cells corresponding to the \keyword{M} words.
		\item Each cell calculates an intermediate state (vector) and passes it to the next cell.
		\item The last cell generates a representation of the text (the probability of the last word's occurrence given the past words).
		\item This representation is introduced to a \keyword{MLP} to estimate the class of the text.
		\item \textbf{Problem}: Technically, during training, words of different sizes cannot be processed with a single matrix operation.
		\item \textbf{Solution}: \textcolor{yellow!30}{A maximum number must be defined; if the words number is lower, padding is added; otherwise, extra words are deleted}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Examples}
	
	\vgraphpage{text_class_RNN_exp.pdf}
	
\end{frame}


\subsection{Sequence classification}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Example}
	
	\vgraphpage{seq_class_RNN_exp.pdf}
	
\end{frame}

% TODO RNN for Text generation
\subsection{Text generation}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Language model}
	
	\vgraphpage{text_gen_RNN_exp.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: GAN-based decoder}
	
	\begin{figure}
		\hgraphpage{text_gen_RNNGAN_exp_.pdf}
		\caption{The illustration of SeqGAN. 
			Left: \textit{D} is trained over the real data and the generated data by \textit{G}. 
			Right: \textit{G} is trained by policy gradient where the final reward signal is provided
			by \textit{D} and is passed back to the intermediate action value via Monte Carlo search. \cite{SeqGAN}}
	\end{figure}
	
\end{frame}


%===================================================================================
\section{Transformer-based architectures}
%===================================================================================

% TODO
\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	\begin{itemize}
		\item Transformers can be used for the 3 tasks
		\begin{itemize}
			\item Both encoder and decoder based ones can be used for classification
			\item encoder based ones are more adapt for sequence classification
			\item decoder based ones are more adapt for text generation
		\end{itemize}
	\end{itemize}
	
\end{frame}


\subsection{Text classification}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: encoder-based (example)}
	
	\begin{center}
		\vgraphpage{text_class_bert_exp.pdf}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: decoder-based (example)}
	
	\hgraphpage{gpt-arch_.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: full-transformer (example)}
	
	\hgraphpage{t5-arch_.pdf}
	
\end{frame}


\subsection{Sequence classification}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: encoder-based (example)}
	
	\begin{center}
		\vgraphpage{text_seq_bert_exp.pdf}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: decoder-based and full-transformer}
	
	\begin{itemize}
		\item Some ideas (\keyword{not based on existing research articles})
		\item Input the text and generate the tags one by one
		\item For full transformers
		\begin{itemize}
			\item the encoder has the sentence as input 
			\item the decoder has the past generated tags as input to output next tag  
		\end{itemize}
		\item For decoder-based architectures
		\begin{itemize}
			\item the decoder has the sentence as input 
			\item a separator
			\item and past generated tags.
		\end{itemize}
	\end{itemize}
	
	
\end{frame}


\subsection{Text generation}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item Decoder-based language models are inherently designed to generate text
		\item Encoder-based language models
		\begin{itemize}
			\item Masked Language Modeling (MLM) for Fill-in-the-Blanks
			\item Training a generator based on their outputs
			\item etc.
		\end{itemize}
	\end{itemize}
	
\end{frame}



\insertbibliography{NLP02}{*}
	
\end{document}

